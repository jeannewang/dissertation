\begin{abstract}

Statistical language models are of great importance to natural language processing tasks. A major problem with current state-of-the-art, neural-based language models is that they tend to be slow to train and test. Neural models can easily take weeks to train on large corpora. 

This dissertation proposes to use hierarchical decompositions of a particular neural model, the log-bilinear model, in order to speed up training and testing times. Using a hierarchical log-bilinear model, training and testing times can be logarithmically sped up. This dissertation focuses on the new combination of the hierarchical log bilinear model and three forms of hierarchical structure: Huffman Trees, Brown Cluster trees, and Recursive ADAPTIVE trees. Additionally, this dissertation explores a new value for model initialization.

We assess the models with a series of experiments that show the pros and cons of each method. A hierarchical log-bilinear model with an ADAPTIVE tree trained on semantic/syntactic word embeddings and with proper initialization is shown to perform $3.89\times$ faster than the factored log-bilinear model with an acceptable entropy loss of $1.93\%$ on the British National Corpus dataset.

\end{abstract}