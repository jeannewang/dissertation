\chapter{Experiments and Results}
In this chapter I will define the experiments and state the experimental results. I will then analyze the results and their implications.

\section{Experiments} \label{sec:experiments}
\paragraph{}
I have conducted experiments to show the performance and speed of the HLBL model with various trees. To be able to compare the models, I use the same parameter values for each model. I use a word and node representation size of 100 dimensions, a $5$-word context (including the target word), a mini batch size of 10,000 and a learning step-size of 0.01. I also use a small regularization constant while training all of the models. Most of the models can be trained faster if the parameters are tuned to the individual model. The models are all trained on the same machine using a single thread. The machine runs at 3.47GHz and has 60GB of RAM.

\subsection{Dataset}
\paragraph{}
For most of my experiments, I use the Penn Treebank dataset. Specifically, I use the Wall Street Journal (WSJ) sections 2-21 for my training set and section 22 for my testing set. There are 950,028 training tokens and 40,117 testing tokens. The training set has been preprocessed to replace any word that occurs less than 6 times with "\_UNK\_". This reduces the vocabulary to 10,531 words. The testing set has also been preprocessed to replace any word not seen in the training set with "\_UNK\_".

%WSJ sections 2-21, with words that occur only once preprocessed out, as a training set. This set has a vocabulary size of 23768. 

%\subsection{Varying Trees}
%I use random Trees, Huffman trees, Brown Cluster trees, ADAPTIVE trees, and Recursive Adaptive Trees with the HLBL model.

\section{Experimental Results}
\paragraph{}
In this section I will briefly describe the experiments and their aims, then display the results and finally analyze and compare the results.

\subsection{Binary Tree Performance by Length and Time}
\paragraph{}
The creation of the trees used in the HLBL model can be done separately from the model. We can look at the trees themselves and see if any of their traits contribute to their performance within the HLBL model.  In Table \ref{tab:trees}, we can see the tree creation statistics. I will not describe tree $T_6$ in this section, as the tree is described in Section \ref{sec:ADAPTIVEQ}.
 
\paragraph{}
The mean code length gives us the average number of decisions that must be made per word. The training time is linear in the mean code length. We would expect the Huffman tree to have the shortest mean length, but the ADAPTIVE(word2vec) tree is actually shorter. This is because the Huffman tree is guaranteed to have the shortest average mean length on the training data unigram frequencies, but the ADAPTIVE(word2vec) tree is created using frequencies learned from the testing data. The Huffman tree has the shortest mean length when the word frequencies agree with those used to create the tree. Given that the Huffman tree has the shortest mean length for the training set data, it is interesting that the Recursive~ADAPTIVE(2,Huffman) tree has a much larger mean length. Hopefully this implies that the Recursive~ADAPTIVE(2,Huffman) tree is giving up short codes for the addition of more useful decision nodes. If we delve further into the code length, we can see how code length affects the test word perplexities. In Table \ref{tab:perplexityCodeLength}, we can see the breakdown of average word perplexities per code length for each tree. The breakdown follows the intuition that longer codes will have worse perplexities. This is due to the increased number of decisions that must be made correctly as the code gets longer. The random tree does not stay true to this intuition, though this is perhaps because of the random tree's tendency to have subtrees that look left-deep or right-deep , whereas the other trees tend to be bushy. Figure \ref{fig:deepvsbushytrees} gives an example of left-deep, bushy, and right-deep trees. The left-deep and right-deep trees have much higher probabilities of going one direction than the other, so long codes may still have high probabilities. The left-deep and right-deep tendencies of the random tree are artifacts of my implementation. In general, random trees should be fairly balanced.

\begin{figure}\centering
\begin{tabular}{ccc}
\Tree [.x     [.x     [.x    {x} {\dots}  ] {\dots}  ] {\dots}  ] &
\Tree [.x   [.x [.x {x} {x} ] [.x {x} {x} ]  ]    [.x [.x {x} {x} ]  [.x {x} {x} ] ] ] &
\Tree [.x    {\dots}  [.x    {\dots}  [.x    {\dots} {x} ]  ]  ] 
\end{tabular} 
\caption{Left-deep, bushy, and right-deep trees}
\label{fig:deepvsbushytrees}
\end{figure}

\paragraph{}
All of the trees have as many or almost as many internal nodes as words. Since the number of internal nodes is not greater than the number of words, the trees are not creating extra un-useful internal nodes. Additionally, the trees are full. A full tree is a tree in which every node, excluding leaf nodes, has two children. Note that fullness is not the same as bushiness. The varying shapes of the trees~(bushiness) accounts for the discrepancies in the mean code length. The training time is also linear in the number of non-leaf nodes, as they correlate directly to rows of the $R$ matrix.
\paragraph{}
We see the greatest differences in the trees in the time it takes to build them. The random tree only depends on the vocabulary so its build time has order $O(|V|)$. Oddly, in Table \ref{tab:trees}, the random tree takes longer to build than the Huffman tree. The Huffman tree has creation order $O(|V|\times \log|V|)$ and should be slower to build than the random tree.  This discrepancy is due to poor handling of random nodes in my code causing random trees to be built slowly. The~Brown Clustering tree is by far the most time intensive tree to build. The Brown Clustering tree takes order $O(|V|^3+n)$, where $n$ is the corpus length. It is slower to build than the Recursive ADAPTIVE($r,t$) trees which require $r-1$ trees and $r-1$ models to be built and trained. The Recursive ADAPTIVE($r,t$) tree has creation order $O ( |V| \times \log|V| \times g + (r-1)\times m + (r-1)\times t)$ where $g$ is the number of iterations it takes the Gaussian Mixture Model to converge, and $m$ is the maximum training complexity of one of the previous recursive HLBL models.

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}C{1.5cm} c C{1.5cm} C{2cm} C{2cm}@{}}\toprule 
Label & Algorithm & Mean Code Length & Non-Leaf Node Count  & Time to Build Tree\\ 
\midrule
$T_1$ & Random & 29.147 & 10530 & 1.2s\\
$T_2$ & Huffman & 26.257 & 10530 & 0.1s\\
$T_3$ & Brown Cluster & 28.388 & 10528 &271600.39s \\
$T_4$ & Recursive ADAPTIVE(3,Random)& 28.218 & 10531 &16735.63s \\
$T_5$ & Recursive ADAPTIVE(2,Huffman)& 32.254 & 10531 &10514.82s\\
\midrule
$T_6$ & ADAPTIVE(word2vec) & 25.823& 10530& 2067.53s\\
\bottomrule
\end{tabular}
\caption{Trees for HLBL model. Recursive ADAPTIVE$(n,tree)$ means it was an ADAPTIVE tree recursively run $n$ times and with an initial HLBL model using $tree$. The ADAPTIVE($x$) tree is an ADAPTIVE tree that was built by clustering together $x$ representations.}
\label{tab:trees}
\end{table*}

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}cccccc@{}}\toprule
Code Length & $T_1$ & $T_2$ & $T_3$ & $T_4$ & $T_5$ \\ \midrule
 3 	&           	  &         	   &  7.26        		 &                 		&                	\\
 4 	&  1.0      	  & 9.32    	   &  20.94       		 &                 		&                	\\
 5 	&  1120.88  	  & 7.19    	   &  32.29       		 &                 		&                	\\
 6 	&  453.48   	  & 24.38   	   &  63.8864     		 &   244.59        		&   1.0          	\\
 7 	&  40.52    	  & 29.15   	   &  239.025     		 &   106.42        		&   145.95       	\\
 8 	&  13873.40 	  & 59.73   	   &  363.81      		 &   253.24        		&   303.32       	\\
 9 	&  4009.30  	  & 113.15  	   &  648.24      		 &   884.43        		&   426.96       	\\
 10	&  11439.68 	  & 256.25  	   &  1401.81     		 &   974.86        		&   1042.68      	\\
 11	&  9299.78  	  & 606.39  	   &  3044.13     		 &   1886.32       		&   2061.74      	\\
 12	&  10495.94 	  & 1377.36 	   &  6088.63     		 &   3904.36       		&   3434.53      	\\
 13	&  251634.94	  & 3645.85 	   &  12131.10    		 &   7362.55       		&   2360.03      	\\
 14	&  122953.48	  & 26992.3 	   &  25847.13    		 &   13369.65      		&   5931.19      	\\
 15	&  81351.98 	  & 22990.0 	   &  40362.92    		 &   27488.21      		&   11402.56     	\\
 16	&  18468.03 	  & 19128.9 	   &  62722.05    		 &   53891.50      		&   38567.71     	\\
 17	&  23142.77 	  & 31569.1 	   &  106630.79   		 &   116346.75     		&   58304.71     	\\
 18	&  17788.92 	  & 1.0     	   &              		 &   225282.24     		&   103076.19    	\\
 19	&  26641.27 	  &         	   &              		 &   461424.90     		&   153507.21    	\\
 20	&  42511.83 	  &         	   &              		 &   1075008.64    		&   1117195.33   	\\
 21	&  64427.15 	  &         	   &              		 &   2251914.54    		&   928355.43    	\\
 22	&  19255.16 	  &         	   &              		 &   5487420.11    		&   2653912.58   	\\
 23	&  35200.19 	  &         	   &              		 &   9891214.67    		&   7303232.64   	\\
 24	&  40986.71 	  &         	   &              		 &   25164936.17   		&   14105262.04  	\\
 25	&  11930.46 	  &         	   &              		 &   49709006.56   		&   33638785.96  	\\
 26	&  18254.02 	  &         	   &              		 &   152108176.37  		&   72852492.39  	\\
 27	&  7165.78  	  &         	   &              		 &   231516402.92  		&   246464544.21 	\\
 28	&  10265.89 	  &         	   &              		 &   168746697.15  		&   584429056.52 	\\
 29	&  5248.084 	  &         	   &              		 &   744083147.85  		&   1007277159.20	\\
 30	&  14582.42 	  &         	   &              		 &   1445162704.19 		&   2197227644.04	\\
 31	&  15846.44 	  &         	   &              		 &   2641639930.40 		&   2742412857.71	\\
 32	&  127.89   	  &         	   &              		 &   1535355556.44 		&   3348260000.0 	\\
 33	&           	  &         	   &              		 &   1.0           		&                	\\
\bottomrule
\end{tabular}
\caption{Average perplexity per code length. }
\label{tab:perplexityCodeLength}
\end{table*}

\subsection{Recursive Adaptive Trees}
\paragraph{}
I ran an experiment to find the optimal level of recursion for the Recursive Adaptive tree. The results for various levels of recursion on the Recursive Adaptive tree with an initial random tree are in Table \ref{tab:recursiveAdaptive}. The~best level of recursion was three with an initial random tree. It is interesting to look at the code length as the number of recursion levels go up. We see that initially, the tree has a low code length, but the average tree length jumps up sharply, on the next recursion level. This correlates to a sharp drop in perplexity. I imagine the tree must be reorganizing itself with more internal nodes to help make more natural decisions. We see that after recursion level two, the average tree length starts decreasing again. I assume this means the opposite trade-off is now happening, in return for fewer decisions overall, the tree gets rid of potentially useful decision nodes. I also run the same experiment with an initial Huffman tree and an initial Brown Cluster tree. The Recursive ADAPTIVE Huffman tree's best perplexity is on recursion level two. The Brown Cluster does not perform better with any level of recursion. I think this implies that the Brown Clustering already has good decision nodes (and good clusters), and that reorganizing the tree actually detracts from the original decision nodes.
\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}cccc@{}}\toprule
Recursion Level & Perplexity & Code Length\\ 
\midrule
$1$ & 118.596 & 25.70\\
$2$ & 105.981 & 33.03\\
$3$ & 104.583 & 28.22\\
$4$ & 110.969 & 25.61\\
$5$ & 109.577 & 25.39\\
\bottomrule
\end{tabular}
\caption{The effect of more recursion levels on the Recursive ADAPTIVE tree with an initial random tree}
\label{tab:recursiveAdaptive}
\end{table*}

\subsection{Varying Representation Dimensionality}
\paragraph{}
The expressivity of the model directly depends on the representation dimensionality. As the size of the vocabulary and training set grows, the expressivity of the model should also grow in order to capture the additional information given by new word combinations and patterns. Mnih and Hinton in \cite{MnihHinton2009} explore varying representation dimensionalities under 100 dimensions. They find that larger dimensions afford better model performance. Mnih and Hinton conclude that 100 dimensions is enough to capture the complexities of the APNews dataset's 17,964 word vocabulary. I looked at representation dimensionalities larger than 100, and found the HLBL model trained on the WSJ dataset benefits from larger word representations. The results can be found in Table \ref{tab:wordRepSize}. Simply by increasing the representation size, even with a random tree, we can see an increase in performance. The increases seem to tail off around 300 dimensions.  The HLBL model may need larger representation sizes than the standard LBL model, as the node representations may need to be larger to capture the complexities of the decision tree.


\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}cccc@{}}\toprule
Word Representation Size & Perplexity using Random Tree\\ 
\midrule
$100$ & 119.339 \\
$150$ & 118.483 \\
$200$ & 113.472 \\
$250$ & 112.519 \\
$300$ & 111.102 \\
\bottomrule
\end{tabular}
\caption{The effect of word representation size on test perplexity.}
\label{tab:wordRepSize}
\end{table*}

\subsection{Comparison against Other Models}
\paragraph{}
I also compared the HLBL model with the various trees from Table \ref{tab:trees} to other language models. All of the HLBL models have the same parameters as described in Section \ref{sec:experiments}. The LBL and Factored LBL models are implemented by Phil Blunsom. The Factored LBL model is the frequency-binned model described in Section \ref{sec:frequencybinning} applied to the LBL model. The model uses 100 frequency classes. The 5-gram with modified Kneser-Ney smoothing is run using the SRILM implementation \cite{Alumae2010}.
The models are allowed to run until their test perplexities stop decreasing. The results are given in Table \ref{tab:languageModelComparison}. The reduction in perplexities and entropy is given in Table \ref{tab:reductionPerplexity}. 

\paragraph{}
The Factored LBL model is the best speedup technique to compare the HLBL model to, as both techniques try to limit the effective vocabulary and also speed up both the training and testing components. The Factored LBL has very good performance and is comparable to the LBL.

\paragraph{}
All of the HLBL models, including using a random tree, perform better than the 5-gram modified Kneser-Ney model,  therefore even simple neural models are more expressive than co-occurence statistics. We can see the performance of all the models compared to the 5-gram model in table \ref{tab:reductionPerplexity}. The best proposed HLBL model in terms of performance uses a Brown Cluster tree ($T_3$) and the best HLBL model considering both time and performance uses the Huffman tree ($T_2$). In Table \ref{tab:perplexityAndSpeedup}, we can see the performance of the Brown Cluster tree and Huffman tree against the LBL and Factored LBL. The Brown Cluster tree HLBL model is pretty good in terms of performance compared to the LBL and Factored LBL models. However, creating the Brown Cluster tree is incredibly slow, and the total time for the HLBL model with a Brown Cluster tree is over an order of magnitude greater than the time for the LBL and Factored LBL models. While the Brown Cluster tree performs as well as any other tree in terms of training time per epoch and testing time, the tree creation time is extremely high. As the overall purpose of this project is to find a tree that is both fast and performs reasonably, the Brown Cluster tree, sadly, does not cut it. Probably the best tree given these criteria, is the Huffman tree. The HLBL model with the Huffman tree is about twice as fast as both the LBL and Factored LBL model, but is also about 15\% worse in terms of perplexity compared to both the LBL and Factored LBL. Still, this can be a price worth paying when it comes to large corpora with hundreds of thousands of words in their vocabulary for a $O(\frac{|V|}{\log|V|})$ speedup versus a $O(\sqrt{|V|})$ speedup. The last two HLBL models use the Recursive ADAPTIVE algorithm ($T_4$ and $T_5$). The Recursive ADAPTIVE algorithm essentially creates Brown Cluster-like trees given existing representations. The resultant trees have semantically/syntactically similar words close together in the tree. At least on a vocabulary of 10K, the Recursive ADAPTIVE algorithm is faster than the Brown Clustering algorithm at creating trees. The Recursive ADAPTIVE tree HLBL models' performances fall between the Brown Cluster and Huffman tree HLBL performances. The Recursive ADAPTIVE algorithm is not fast enough however to be worth using over the simple Huffman tree. 

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}C{3cm}ccC{2cm}C{2cm}cc@{}}\toprule
Model & Tree & Perplexity & Epochs Till Convergence & Training Time Per Epoch & Testing Time & Total Time\\ 
\midrule
 HLBL & $T_1$ &119.339 & 71& 40.19s & 0.57s& 2855.26s\\
 HLBL & $T_2$ &115.982 & 99& 28.93s & 0.47s & 2864.64s\\
 HLBL & $T_3$ &101.563 & 118& 28.98s & 0.49s & 275020.52s \\
 HLBL & $T_4$ &104.583 & 122& 32.48s & 0.52s & 20698.71s\\
 HLBL & $T_5$ &106.992 & 114& 37.08s & 0.58s& 14742.52\\
 LBL& - &97.21 &62 &106.98s & 21.47s& 6654.23s\\
 Factored LBL & - &98.42&101 & 47.19s & 0.56s & 4766.75s \\
 5-gram modified KN & - &122.752& 1 & 5.51s & 0.348s & 5.51s\\
\bottomrule
\end{tabular}
\caption{Comparison of HLBL model with various trees and other language models on WSJ dataset}
\label{tab:languageModelComparison}
\end{table*}

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}ccC{2cm}C{2cm}@{}}\toprule
Model & Tree & Reduction in Perplexity & Reduction in Entropy \\
\midrule
 HLBL & $T_1$ &2.78\% & 0.58\%\\
 HLBL & $T_2$ &5.12\% & 1.17\% \\
 HLBL & $T_3$ &17.24\% & 3.93\%  \\
 HLBL & $T_4$ &14.8\% & 3.32\%\\
 HLBL & $T_5$ &12.84\% & 2.85\%\\
 LBL& - & 20.81\% &4.84\%\\
 Factored LBL & - &19.82\%&4.59\% \\
\bottomrule
\end{tabular}
\caption{Reduction in perplexity and entropy compared to the 5-gram modified Kneser-Ney smoothed model on WSJ dataset.}
\label{tab:reductionPerplexity}
\end{table*}

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}ccC{1.75cm}C{1.75cm}C{1.75cm}C{1.75cm}C{1.75cm}C{1.75cm}@{}}\toprule
Model & Tree
& Increase in Perplexity Compared to LBL & Increase in Entropy Compared to LBL
& Increase in Perplexity Compared to Factored LBL & Increase in Entropy Compared to Factored LBL
& Time Compared to Factored LBL & Time Compared to Factored LBL
\\
\midrule
 HLBL & $T_2$ & 16.19\% & 3.71\% & 15.4\% & 3.45\% & $2.32\times$ faster & $1.66\times$ faster\\
 HLBL & $T_3$ & 4.28\% & 0.95\% & 3.09\% & 0.68\% &$41\times$ slower & $57\times$ slower\\
\bottomrule
\end{tabular}
\caption{HLBL models compared against the LBL and Factored LBL models on the WSJ dataset.}
\label{tab:perplexityAndSpeedup}
\end{table*}


\subsection{Visualization} \label{sec:tsne}
\paragraph{}
To see what kinds of word representations are learned by the HLBL models, I use the t-SNE visualization technique described in \cite{Maaten2008} and implemented by Laurens van der Maaten. Using t-SNE, I visualized each context word representation in matrix $R$ on a two-dimensional map. The context word is displayed on top of the two-dimensional coordinates. This should give us an idea of the types of word similarities that are learned. Additionally, I use t-SNE to visualize words learned by the tree in the matrix $Q$. Since $Q$ vectors do not directly correspond to words, I use the $Q$ node directly above a leaf word or leaf words to represent the word, or word-pair. After t-SNE is applied to the $Q$ vectors, the word or word-pair is then displayed on top of the two-dimensional location representing the $Q$ vector.
\paragraph{}
As the full visualization over all 10K vocabulary words is large, I have selected some areas from the $Q$ word-cloud in Figure \ref{fig:Qcloud} and some areas from the $R$ word-cloud in Figure \ref{fig:Rcloud}. The Factored LBL model's $R$ and $Q$ matrix are displayed as a baseline. A fuller version of the visualizations can be found in Appendix B. 
\paragraph{}
The $Q$ matrix holds the learned vector representations for the context words. I found that the HLBL models, using the random tree ($T_1$) and the Huffman tree ($T_2$), learn some $Q$ word representations that make semantic or syntactic sense, whereas the models with the other three trees ($T_3,T_4,T_5$) do not. In Figure \ref{fig:Qcloud} we can see that the HLBL model with the random tree clusters together numbers in the first example, and pronouns and prepositions in the second example. The HLBL model with the Huffman tree clusters together names and places. I think this may be because the random and Huffman tree HLBL models are forced to learn word "meanings" as the tree does not automatically group together similar words. For the other three models, the trees already separate out the word meanings, and so instead of focusing on learning the word meanings, the model focuses on learning the decision paths. The other three models learn word representations that are optimized for correctly predicting the path through the tree. Since the other three models have better perplexities than the random and Huffman tree HLBL models, I assume this is because the other three trees start off with better word clusterings than is learned by the HLBL model with random or Huffman trees. This is supported by the fact that the $Q$ word-clouds are still uniform looking and unclustered (with some exceptions), for the random and Huffman HLBL models. Additionally, the other three models have the benefit of learning the paths better.
\paragraph{}
The $R$ matrix holds the learned vector representations for the decision tree nodes. In Figure \ref{fig:Rcloud}, we see that the HLBL models with the random and Huffman trees seem to learn some semantic or syntactic patterns in their $R$ matrices, whereas the other three trees do not. The two examples for the random tree cluster together numbers and cluster together names. The Huffman tree seems to have learned verbs and prepositions. In the first example for the Huffman tree, it looks like the left word is a present tense verb, and the right word is a past tense verb, with the exception of 'pressing'. Since the $Q$ and $R$ matrices are learned jointly, it makes sense that the $R$ matrices also show the same semantic/syntactic emphasis in learning. In comparison, the Brown Cluster tree and the Recursive Adaptive trees have round $R$ word-clouds. This means that the $R$ matrices for these three trees are close to multi-dimensional Gaussians, and do not help much with the models. It seems that the HLBL model does not learn $R$ representations when given trees that cluster semantically or syntactically words together. This seems to imply that most of the model parameters are captured in the $Q, C$ and $B$ matrices for HLBL models with such trees.

\begin{figure}[p]
\centering
\begin{tabular}{@{}m{1cm}m{1cm}m{3.5cm}m{3.5cm}m{3.5cm}@{}}
HLBL & $T_1$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_random_it71_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_random_it71_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_random_it71_small2.png}
\\
HLBL & $T_2$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Huff_iter132_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Huff_iter132_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Huff_iter132_small2.png}
\\
HLBL &$T_3$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_iter_118_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_iter_118_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_iter_118_small2.png}
\\
HLBL & $T_4$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptiveR3_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptiveR3_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptiveR3_small2.png}
\\
HLBL & $T_5$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive-huffR1_it114_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive-huffR1_it114_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive-huffR1_it114_small2.png}
\\
Factored LBL model &&
\includegraphics[width=0.15\textheight]{./images/tsne/Q_factored_it7_step0_05_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_factored_it7_step0_05_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_factored_it7_step0_05_small2.png}
\end{tabular}
\caption{$Q$ matrices for various trees projected into two dimensions. The left-most image shows the general shape of all of the words together, and the center and right-most image show close-ups of the word-cloud.}
\label{fig:Qcloud}
\end{figure}

\begin{figure}[p]
\centering
\begin{tabular}{@{}m{1cm}m{1cm}m{3.5cm}m{3.5cm}m{3.5cm}@{}}

HLBL & $T_1$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_random_it71_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_random_it71_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_random_it71_small2.png}
\\
HLBL & $T_2$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Huff_iter132_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Huff_iter132_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Huff_iter132_small2.png}
\\
HLBL & $T_3$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_iter_118_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_iter_118_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_iter_118_small2.png}
\\
HLBL & $T_4$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptiveR3_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptiveR3_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptiveR3_small2.png}
\\
HLBL &$T_5$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive-huffR1_it114_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive-huffR1_it114_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive-huffR1_it114_small2.png}
\\
Factored LBL model &  &
\includegraphics[width=0.15\textheight]{./images/tsne/R_factored_it7_step0_05_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_factored_it7_step0_05_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_factored_it7_step0_05_small2.png}
\end{tabular}
\caption{$R$ matrices for various trees projected into two dimensions. The left-most image shows the general shape of all of the words together, and the center and right-most image show close-ups of the word-cloud.}
\label{fig:Rcloud}
\end{figure}

\subsection{word2vec $Q$ Initialization} \label{sec:ADAPTIVEQ}
\paragraph{}
Another experiment I tried was using better word representations to initialize $Q$. After observing that the HLBL model with the Brown Cluster tree and the Recursive ADAPTIVE trees do not seem to learn semantically or lexically motivated clusters in their $Q$ matrices, I thought that by improving the initialization of the $Q$ matrices, I could push the models to learn better $Q$ matrices. 
\paragraph{}
I use the word2vec toolkit representations implemented by Mikolov et al., to initialize my $Q$ matrix. The word2vec toolkit is based off of the ideas in \cite{Mikolov2013}. I create word2vec representations with a skip-gram model, the hierarchical soft-max, a word-window of 5, down-sampling of .001 and a representation dimensionality of 100. The representation creation takes 2.309 seconds to run with 12 threads on the WSJ dataset.  I test the new $Q$ initialization with the HLBL model using a Brown Cluster Tree, an ADAPTIVE tree, and also a Huffman tree. The results for the HLBL model with word2vec $Q$ initialization are in Table \ref{tab:brownWord2vec}.

\paragraph{}
The $Q$ initialization with the word2vec representations improves the perplexity of the HLBL model with a Brown Cluster tree~($T_3$) by 2.79\% and the entropy by 0.61\%. We also see that with better $Q$ initialization, the HLBL model with a Brown Cluster tree performs almost as well as the LBL model. The perplexity of the Brown Cluster tree is only -0.43 away from the LBL's perplexity on the WSJ dataset. Unfortunately, this still suffers from the long creation time of the Brown Cluster tree.
\paragraph{}
To avoid the long creation time of the Brown Cluster tree, I also tried using an ADAPTIVE tree~($T_6$). The ADAPTIVE tree tries to cluster similar word representations together in the tree. If the word representations capture semantic and syntactical information, then the ADAPTIVE tree acts much like the Brown Cluster tree. To keep the tree creation time low, I did not run the ADAPTIVE algorithm recursively. Using the same word2vec representations described above, I create an ADAPTIVE tree, and also initialize the $Q$ matrix to the word2vec representations. The ADAPTIVE tree does not perform as well as the Brown Cluster tree, but is faster to create. The ADAPTIVE tree is also faster overall than the LBL model but not the Factored LBL on the WSJ dataset.

\paragraph{}
I also tried initializing the $Q$ matrix with the word2vec representations for the Huffman tree~($T_2$) HLBL model with no performance gain. The Huffman tree model does a good job learning the context words even without sensible initialization. 

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{cccccc}\toprule
Tree & Perplexity & Epochs & Training Time Per Epoch & Testing Time & Total Time\\ 
\midrule
$T_2$ & 115.786 & 81 & 26.31s &0.46s & 2133.98s \\
$T_3$ & 97.64 & 115& 30.92s & 0.49s& 275022.82s\\
$T_6$& 104.477 & 83& 35.46s & 0.56s& 5013.57s\\
\bottomrule
\end{tabular}
\caption{HLBL model with $Q$ initialized to word2vec representations on WSJ dataset.}
\label{tab:brownWord2vec}
\end{table*}

\subsection{Visualization of HLBL Models with word2vec $Q$ Initialization}
\paragraph{}
I ran the t-SNE visualizations described in Section \ref{sec:tsne} over the HLBL models with $Q$ initialized to the word2vec representations. Note that I did not include the Huffman tree with word2vec $Q$ initialization since the model does not seem to be better than its non-word2vec initialized counterpart. The results can be seen in Figures \ref{fig:QcloudWord2Vec} and \ref{fig:RcloudWord2Vec}. We see that the $Q$ word-clouds make more semantically and syntactically similar clusters unlike the models without the word2vec $Q$ initialization. This is exactly the result we were looking for. The $R$ word-clouds remain largely unchanged. 

\begin{figure}[p]
\centering
\begin{tabular}{@{}m{2cm}m{3.5cm}m{3.5cm}m{3.5cm}@{}}
$T_3$&
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_QinWord2Vec_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_QinWord2Vec_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_QinWord2Vec_small2.png}
\\
$T_6$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive_QinWord2Vec_RinWord2Vec_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive_QinWord2Vec_RinWord2Vec_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive_QinWord2Vec_RinWord2Vec_small2.png}
\end{tabular}
\caption{$Q$ matrices for HLBL models with $Q$ initialized to word2vec representations. The left-most image shows the general shape of all of the words together, and the center and right-most image show close-ups of the word-cloud.}
\label{fig:QcloudWord2Vec}
\end{figure}

\begin{figure}[p]
\centering
\begin{tabular}{@{}m{2cm}m{3.5cm}m{3.5cm}m{3.5cm}@{}}
$T_2$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_QinWord2Vec_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_QinWord2Vec_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_QinWord2Vec_small2.png}
\\
$T_6$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive_QinWord2Vec_RinWord2Vec_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive_QinWord2Vec_RinWord2Vec_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive_QinWord2Vec_RinWord2Vec_small2.png}
\end{tabular}
\caption{$R$ matrices for HLBL models with $Q$ initialized to word2vec representations. The left-most image shows the general shape of all of the words together, and the center and right-most image show close-ups of the word-cloud.}
\label{fig:RcloudWord2Vec}
\end{figure}


\subsection{Large Vocabulary}
\paragraph{}
Real languages have much larger vocabularies than the WSJ dataset. The Oxford English Dictionary currently has 171,476 full word entries for English \cite{OED}. For a morphologically rich language, the vocabulary size can easily exceed that of English.
\paragraph{}
The HLBL model really shines in terms of speed with large vocabulary sizes. The speedup using the HLBL model is order $O(\frac{|V|}{\log|V|})$. To highlight this, I also run some experiments with a subset of the British National Corpus (BNC). I use a random 120,000 lines of the BNC, where 100,000 lines are randomly assigned to the training set and the other 20,000 lines are assigned to the test set. Due to an error in generating the test set, 80,000 of the training set sentences are also in the test set. This simply means the testing perplexities are better than they really should be for all models. Since the BNC dataset is only being used to highlight the speed difference of models, I did not rerun the experiments. The duplication of training examples in the BNC test set makes the test set larger, and actually helps highlight the testing speed difference of the models. There are 2,141,639 training tokens and 2,138,171 testing tokens. The training set has a vocabulary size of 84,439 (including "\_UNK\_"). The test set is preprocessed to replace any word not seen in the training set with "\_UNK\_". We can see on the 85K word vocabulary where non-HLBL models slow down immensely. Note that while we can see slow-downs with an 85K vocabulary, the BNC dataset is still very small in comparison to data used in commercial applications. An industrial language model will use corpora with hundreds of thousands of words in their vocabulary and billions or trillions of training tokens. The training time will generally dominate the tree creation time on such large corpora.

\paragraph{}
Using the 85K vocabulary BNC dataset, I run the HLBL model with two different trees, the Huffman tree and also the ADAPTIVE(word2vec) tree with Q initialized to the word2vec representations. The BNC word2vec representations take 4.960s to create running with 12 threads. The results are shown in Table \ref{tab:largeVocabulary}.
\paragraph{}
The results show why a hierarchical speed up technique is useful for computing language models over large corpora. The Factored LBL model that performs well in terms of perplexity and speed on the 10K WSJ vocabulary, performs much slower with the 85K BNC vocabulary. The Factored LBL model's training and testing times scale linearly with the vocabulary size. The HLBL model on the other hand scales logarithmically.  We see the HLBL models are faster both in training and testing than the Factored LBL model. The HLBL model with the Huffman tree is $4.46\times$ faster to compute than the Factored LBL model. Though there is a perplexity loss of $20.72\%$ and an entropy loss of $4.46\%$. The HLBL model with the ADAPTIVE(word2vec) tree and $Q$ initialized to word2vec representations is $3.89\times$ faster to compute than the Factored LBL model and has a perplexity loss of $9.34\%$ and an entropy loss of $1.93\%$ This makes the HLBL model with the ADAPTIVE tree and word2vec $Q$ initialization the best of the HLBL models I explored.

%\begin{table*} \centering
%\ra{1.3}
%\begin{tabular}{@{}ccC{2cm}cc@{}}\toprule
%Model & Perplexity & Training Time Per Epoch & Testing Time & Total Time\\ 
%\midrule
% LBL&105.442 &111.05s & 21.47s & 6906.57s \\
% Factored LBL &108.802& 71.05s & 0.60s & 7887.15s\\
% HLBL with Huffman Tree &131.913& 33.26s &0.48s &3492.79s \\
%\bottomrule
%\end{tabular}
%\caption{The effect of large vocabularies on training and testing times}
%\label{tab:largeVocabulary}
%\end{table*}

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}C{2cm}ccccC{2cm}C{2cm}C{2cm}@{}}\toprule
Model & Tree & Q init & Perplexity & Epochs & Training Time Per Epoch & Testing Time & Total Time\\ 
\midrule
 Factored LBL & - & Gaussian &144.32& 112 & 479.37s & 72.14s & 53761.58s\\
 HLBL& $T_2$ & Gaussian &182.06& 117& 102.7s &30.18s &12047.61s \\
 HLBL & $T_6$ & word2vec &159.187& 106& 129.81s &38.32s &13803.14s \\
\bottomrule
\end{tabular}
\caption{The effect of a large vocabulary (85K) from the BNC dataset on training and testing times}
\label{tab:largeVocabulary}
\end{table*}
  
  
  
