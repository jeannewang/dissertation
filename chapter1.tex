
\chapter{Introduction}
\paragraph{}
In the field of computational linguistics, statistical language modeling is a cornerstone task. The use of language models  is universal in applications such as machine translation and speech recognition.  Improving upon language models can improve the performance of these important applications and future applications that involve language models.
\section{Motivation}
\paragraph{}
The state-of-the-art language models are all neural-based models. In spite of this, n-gram models are still incredibly prevalent and widely used. Neural-based models are unattractive to use due to their long training and testing times. The training of a large neural network based model can take weeks, as opposed to the minutes a n-gram model takes. While neural network models may never be as fast as n-gram models to train and test, these models can become much more attractive with shorter training and testing times. If a user only had to wait hours for his model instead of weeks, neural models become a much more persuasive option.
\paragraph{}
Additionally, given the large amount of buzz and research around \emph{deep-learning}, neural models are only going to get better in the future. It becomes paramount to find ways to speed up these models for both research and commercial usage.
\paragraph{}
One way to speed up these models is to apply a hierarchical structure to the neural model. This dissertation will explore using such a hierarchical model and new forms of hierarchy.

\section{Objectives}
\paragraph{}
One simple neural-based model with good performance is the log-bilinear model \cite{MnihHinton2007}. I would like to apply a hierarchical structure to it in the form of the hierarchical log-bilinear model \cite{MnihHinton2009}. The hierarchical structure, or tree, can be interchanged freely. I would like to explore various trees to improve the model performance. The objective of this project is to explore novel trees with the hierarchical log-bilinear model in order to find a tree that is fast to train, test and create and also performs well in comparison to the standard log-bilinear model.

\section{Contributions}
\paragraph{}
In this dissertation, I explore the combination of three trees with the hierarchical log-bilinear model: the Huffman tree, the Brown Cluster tree and the Recursive ADAPTIVE tree. These three trees have not been used before with the hierarchical log-bilinear model. I find that the Brown Cluster tree is the best performing tree in terms of perplexity of the three, but is very slow to create. The Huffman tree on the other hand is fast to create, but does not perform very well in terms of perplexity. The Recursive ADAPTIVE tree is in the middle of the pack; it is relatively slow to create but has better performance than the Huffman tree. To improve the performance of the ADAPTIVE tree model, I use word2vec representations to initialize the context word representation parameters. This proves to be a good combination and results in a model that is both fast and accurate when applied to large corpora. The new model is $3.89\times$~faster than the factored log-bilinear model but is 1.93\% worse in terms of entropy.

\section{Outline}
\paragraph{}
In Chapter 2 of this dissertation I will cover language models including the n-gram model, maximum entropy model, neural probabilistic model, feed-forward neural network model, recurrent neural network model and the log-bilinear model. Additionally, I will describe extensions to these models and techniques to evaluate models.

\paragraph{}
In Chapter 3, I will cover speedup techniques for language models and describe why I have decided to focus on using a hierarchical method. These speedup techniques include class-based methods, hierarchical-based methods, gradient approximation methods, vocabulary truncation, diagonalizing context matrices, and also switching the training order.

\paragraph{}
In Chapter 4, I will explain my proposed method including the model definition and the tree definitions. The proposed model is the hierarchical log-bilinear model and the trees explored are the Huffman tree, the Brown Cluster tree, and the Recursive ADAPTIVE tree.

\paragraph{}
In Chapter 5, I describe engineering tips and tricks for implementing the model. These include model initialization, tree building, gradient checking and variable caching.

\paragraph{}
In Chapter 6, I describe the datasets used, the experiments undertaken, the reasoning behind each experiment and also state the experimental results. The results are then compared and analyzed. 

\paragraph{}
In Chapter 7, I derive conclusions for the project and also recommend directions for future work.

\paragraph{}
Lastly, I include three appendices. Appendix A gives a derivation of the gradients used in training the model. Appendix B holds visualizations of the word and node embeddings learned by the models. Appendix C holds a copy of the most relevant parts of my C++ implementation of the model.
