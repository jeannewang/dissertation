\chapter{Proposed Model and Trees}
In this chapter, I will cover the details of the hierarchical log-bilinear model including the model definition, the method of training and also the method of testing. I will also cover the creation methods of the three novel trees I am exploring: the Huffman tree, the Brown Cluster tree and also the Recursive ADAPTIVE tree.

\section{Proposed Model} \label{sec:proposedModel}
\paragraph{}
The hierarchical log-bilinear model (HLBL) proposed by Andriy Mnih and Geoffrey Hinton in \cite{MnihHinton2009} is the starting point for my project. Mnih and Hinton make it clear that varying the binary trees used for the hierarchical representation of the vocabulary can make a significant difference in the performance and speed of the model. To be specific, "trees
that are well-supported by the data and are reasonably well-balanced so that the resulting models
generalize well and are fast to train and test" should be used \cite[pg. 5]{MnihHinton2009}. While Mnih and Hinton propose  effective trees to use in the HLBL model, I will explore the effects of three new binary trees. I intend to test out Huffman encoding trees, Brown Clustering trees, and Recursive ADAPTIVE trees.
\subsection{Motivation}
\paragraph{}
The HLBL model can be made faster and more accurate by using good decision trees. There is a trade-off between the expressiveness of the model's learned node representations and the number of decisions that must be made per word. In the ideal situation, the learned node representations would exactly capture their constituent words' semantic/lexical properties and also the correct decision path through the tree. In absence of the ideal, we would like to make the decision process easier to learn so the representations can capture the semantic/lexical properties better. Minh and Hinton do a good job creating trees that alleviate this burden, but I would like to explore even simpler trees in the hope that they can also capture useful decisions. Hopefully, trees that are simple to create and understand will also perform well in the HLBL model. 

\subsection{Model Definition}
\paragraph{}
This dissertation uses the HLBL model described in Section \ref{sec:HLBL}. The HLBL treats each word probability as the product of the probabilities of binary decisions through a tree.

\subsubsection{Maximum Likelihood Objective Function}
\paragraph{}
We would like to find a model that is good at predicting words given a context; one way of doing this is by maximizing the probabilities of the observed words. This is also known as maximizing the \emph{likelihood}. The likelihood of seeing the training set given a model is the~joint probability of all words in the training set. We choose the likelihood to be our objective function~$J$ which is given by the following:
\begin{align}
&J(\theta;w_1,\dots, w_m) = \prod_{i=1}^{m} P(w_i | w_{i-(n-1)},\dots, w_{i-1}) \label{eq:likelihood}
\\
&P(w_i | w_{i-(n-1)},\dots, w_{i-1})  \approx \prod_j P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1}) \label{eq:likelihoodDecisions}
\\ 
&P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1}) =  
\begin{cases}
  \sigma( \hat{r}^T q_{j} +b_{j}) 	      & \text{if } d_j(w_i)  = 1 \\
 1- \sigma( \hat{r}^T q_{j} +b_{j})     & \text{if } d_j(w_i) = 0
  \end{cases} \nonumber
\\
&\hat{r} = \sum_{i=1}^{n-1} C_i r_{w_i} \nonumber
\end{align}
where $\theta$ is shorthand for all of the parameters of the model ($Q,R,B$, and $C_i$ for all context positions), $m$ is the size of the training set, $d_j(w_i)$ is the binary decision for word $w_i$ at node~$j$,  $q_j$ is the vector representation for node~$j$ stored in a row of $Q$, $b_{j}$ is a scalar bias term for node $j$ stored in $B$, $r_{w_i}$ is the vector representation for context word $w_i$ stored in a row of $R$, and $C_i$ is the matrix that corresponds to the interaction between the $i$th context word and the target node.  The $\hat{r}$ term can be thought of as a prediction vector for $w_i$. Also $\sigma(x)$ is the sigmoid function. 
\paragraph{}
In Equation \ref{eq:likelihoodDecisions}, the product over $j$ corresponds to the product of all binary decision probabilities along the path in the tree to word $w_i$. Each binary decision is made at a node $j$. Additionally, note the use of the simplifying assumption that binary decision predictions are independent of each other in Equation \ref{eq:likelihoodDecisions}.
\paragraph{}
Given that we are maximizing the likelihood, we would like to choose parameters, such that $\hat{\theta} = \argmax_\theta J(\theta;w_1,\dots, w_m)$. The set of parameters $\hat{\theta}$ is known as the \emph{maximum likelihood estimator} of $\theta$ \cite{Elkan2013}.  In our actual usage of the objective function we will modify it slightly. We can take the negative natural logarithm of the objective function to get the \emph{negative log likelihood}. For mathematical and computational convenience, we will be minimizing the negative log likelihood which is equivalent to maximizing the likelihood, since logarithm is a strictly monotonically increasing function \cite{Elkan2013}. We minimize the negative log likelihood such that  $\hat{\theta} = \argmin_\theta -\ln(J(\theta;w_1,\dots, w_m))$.
From Equations \ref{eq:likelihood} and \ref{eq:likelihoodDecisions}, it follows that:
\begin{align}
&-\ln(J(\theta;w_1,\dots, w_m)) = \sum_{i=1}^{m} -\ln(P(w_i | w_{i-(n-1)},\dots, w_{i-1}))
\\
&-\ln(P(w_i | w_{i-(n-1)},\dots, w_{i-1})) \approx \sum_j - \ln(P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1})) 
\\
&- \ln(P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1})) = 
\begin{cases}
  \ln (1 + e^{-\hat{r}^T q_{j} -b_{j}}) 	      & \text{if } d_j(w_i)  = 1 \\
  \ln (1 + e^{\hat{r}^T q_{j} +b_{j}})    & \text{if } d_j(w_i) = 0
  \end{cases} \nonumber
\end{align}

\paragraph{}
We use the logarithm of the objective function because it changes the products into sums which are easier to calculate and are less likely to overflow or underflow on a computer.

\subsubsection{Probability Mass Function}
\paragraph{}
The word probability distribution is given by a \emph{probability mass function} (PMF) that sums up to one. We can see that the PMF must sum to one if we realize each binary decision has a sigmoid output function that maintains the probability mass. The sigmoid function is displayed in Figure \ref{fig:sigmoid} and is given by $\sigma(x) = \frac{1}{1+e^{-x}}$. Since the sigmoid function is always between $0$ and $1$, we can treat each binary decision output as a probability. If we sum over both possibilities for a binary decision, the probability will be 1. We can therefore think of each decision as dividing up the probability mass, but never losing any mass. 
In Figure \ref{fig:btree}, the four word probabilities $(\frac{1}{3},\frac{1}{3}, \frac{1}{12}, \frac{1}{4})$ would each be the product of decision probabilities on the path to their respective word. These word probabilities sum up to one.

\begin{figure}
\centering
\includegraphics[height=200px]{./images/sigmoid.eps}
\caption{Sigmoid function}
\label{fig:sigmoid}
\end{figure}

\begin{figure}
\Tree [.$dp=1$ [.$dp=\frac{2}{3}$ [.{$dp=\frac{1}{2}$} {$wp = \frac{1}{3}$} ]  [.{$dp=\frac{1}{2}$} {$wp = \frac{1}{3}$} ]] [.$dp=\frac{1}{3}$ [.{$dp=\frac{1}{4}$} {$wp = \frac{1}{12}$} ] [.{$dp=\frac{3}{4}$} {$wp = \frac{1}{4}$} ] ]  ]
\caption{Binary decision probabilities in a tree. dp is a decision probability and wp is a word probability.}
\label{fig:btree}
\end{figure}

\subsubsection{Regularization}
\paragraph{}
Standard maximum likelihood estimation solutions tend to overfit. This over fitting comes from the fact that parameter values are unbounded, so in the limit, the parameter values may go to infinity when maximizing the overall objective function. To stop parameters from going to infinity, \emph{regularization} is added. Regularization simply adds a penalty to large parameter values. Many types of penalties exist, but I will be using the squared $L_2$ norm ($||x||^2_2$) as a penalizer. The maximum likelihood estimator then becomes:
\begin{align}
&\hat{\theta} = \argmin_\theta -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2
\\
& ||\theta||^2_2 = \sum_{j=1}^d \theta_j^2 \nonumber
\end{align}
Where $\mu$ is the regularization parameter that controls the tradeoff between maximizing J and minimizing the parameter values and $d$ is the length of the parameter vector $\theta_j$ \cite{Elkan2013}. Note that with regularization, our estimate is a \emph{maximum a posteriori} estimation using a Gaussian prior with mean 0 and variance $\sqrt{\frac{1}{2\mu}}$.

\subsubsection{Similarity to Logistic Regression}
\paragraph{}
The HLBL model is actually similar to \emph{logistic regression}. We can think of HLBL as logistic regression with knowledge of context words. 
The standard logistic regression model looks like:
\begin{align}
P(y =1 | x; \alpha, \beta) = \sigma \left( \sum_{j=1}^d \beta_j x_j + \alpha \right)
\end{align}
If we reformulate the logistic regression model to look like the HLBL:
\begin{align}
P( b_j(w_j) = 1 | w_i; b_j, q_j) = \sigma \left( \sum_{j=1}^d q_j w_j + b_j \right)
\end{align}
We can then compare it against the HLBL model:
\begin{align}
P(d_j(w_i) = 1 | q_j, w_{i-(n-1)},\dots, w_{i-1}) =  \sigma( (\sum_i C_i r_{w_i})^T q_{j} +b_{j})
\end{align}
We see that most of the two models are the same, the main difference being the addition of the $C_i$ matrices.  The $C_i$ matrices hold parameters that deal with the interactions between context words and nodes in the tree. The $q_j$ term also plays a different role. In the logistic regression model, $q_j$ is a weight for word $w_j$, whereas in the HLBL model, $q_j$ is a vector representation for a node in the tree. Still, the models are similar and we can think of HLBL as logistic regression for nodes in the tree, weighted by context prediction vectors.


\subsection{Model training}
\paragraph{}
To train the HLBL model, we need a way to minimize the parameters $\theta$. One way to minimize the parameters is through an iterative scheme that moves closer to the (local) minimum on each step called gradient descent. The HLBL objective function is smooth and so is easily differentiable. However, it is not convex so it may have more than one minimum. 
\subsubsection{Gradient Descent}
\paragraph{}
\emph{Gradient descent} is a first-order optimization algorithm. The basic idea behind gradient descent is to move in the direction of the negative gradient on each time step until the gradient becomes zero. When the gradient is zero, we have hit a local minimum. Since the MLE parameters minimize $-J(\theta)$, we can apply gradient descent to the parameters $\theta$. Of course, even if we are constantly moving in the direction of the gradient, we may be moving towards it in too small steps or too large steps. With too small a step size, we converge to the minimum very slowly. With too large a step size, we can overshoot the minimum and actually get farther and farther away from the minimum. The learning rate $\gamma$, controls the step size at which we move toward the minimum. The learning rate $\gamma$ is typically fixed, and found by trial and error \cite{Elkan2013}.
A single step of gradient descent is given by:
\begin{align}
\theta := \theta - \gamma \nabla_\theta \left( -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2 \right) \label{eg:gradientDescent} 
\end{align}
where $m$ is the total number of training examples.

\subsubsection{Gradients}
\paragraph{}
We can break up Equation \ref{eg:gradientDescent} into its constituent gradients for parameters $Q,R,B$ and $C_i$ for all context indices. The gradient derivation can be found in Appendix A.

Here are the gradients after every decision $d_j(w_i)$:
\begin{align}
\nabla_{b_j} \left( -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2 \right)  = &  \left( P \left(d_j(w_i) = 1 | h \right) - d_j(w_i) \right) +2\mu b_j  \label{eq:gradients}
\\
\nabla_{q_j} \left( -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2 \right)  = &\left( P \left(d_j(w_i) = 1 | h \right) - d_j(w_i) \right)\hat{r} +2\mu q_j \nonumber
\\
\nabla_{r_{w_i}} \left( -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2 \right)  = & \left( P \left(d_j(w_i) = 1 | h \right) - d_j(w_i)\right) C_i^T q_j   +2\mu r_{w_i} \nonumber
\\
\nabla_{C_i} \left( -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2 \right)  = &\left( P \left(d_j(w_i) = 1 | h \right) - d_j(w_i) \right) r_{w_i}q_j^T  +2\mu C_i \nonumber
\\
P \left(d_j(w_i) = 1 | h \right)= & P \left(d_j(w_i) = 1 | q_j, w_{i-(n-1)},\dots, w_{i-1} \right) \nonumber
\end{align}
There terms $q_j$ and $b_j$ should be updated per decision but the terms $r_{w_i}$ and $C_i$ should only be updated per word. Here are the word gradients:
\begin{align}
& \nabla_{r_{w_i}} \left( -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2 \right)  =  C_i^T \sum_j \left( P \left(d_j(w_i) = 1 | h \right)  - d_j(w_i) \right)q_j +2\mu r_{w_i} \nonumber
\\
& \nabla_{C_i} \left( -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2 \right)  =  r_{w_i} \sum_j \left( P \left(d_j(w_i) = 1 | h \right) - d_j(w_i) \right) q_j^T  +2\mu C_i
\end{align}
Gradient descent updates the parameters once for every full sweep though all of the training examples. A single iteration of gradient descent has time complexity $O(M\times D)$, where $M$ is the number of training examples and $D$ is the dimensionality of the feature vector \cite{Elkan2013}. 

\subsubsection{Stochastic Gradient Descent}
\paragraph{}
The \emph{stochastic gradient descent} algorithm is almost the same as gradient descent except that all of the parameters are updated after each training example. Also, the training examples are trained in random order. Intuitively, stochastic gradient descent approximates the gradient over all examples, by using a single random example. Stochastic gradient decent can converge to good parameter values in much fewer training iterations than gradient descent. A single iteration of stochastic gradient descent takes order $O(M\times F \times D)$ time where $M$ is the number of training examples, $D$ is the dimensionality of the feature vector and $F$ is the average number of non-zero features per example \cite{Elkan2013}. 
A stochastic gradient descent update looks like:
\begin{align}
\theta := \theta - \gamma \nabla_\theta \left( -\ln(P(w_i | w_{i-(n-1)},\dots, w_{i-1})) +  \mu ||\theta||^2_2 \right)
\end{align}
\subsubsection{Mini-batches}
\paragraph{}
A compromise between gradient descent and stochastic gradient descent is to use \emph{mini-batches}. Instead of updating parameters after each example like stochastic gradient descent, or after all examples like gradient descent, one updates the parameters after a mini-batch of examples. A \emph{mini-batch} is a small selection of examples. The size of a mini-batch is typically a small percentage of the total parameters, on the order of a few hundred or thousand depending on how many total parameters there are.
\paragraph{}
A single step of stochastic gradient descent using mini-batches looks like:
\begin{align}
\theta := \theta - \gamma  \sum_{i=1}^{k} \nabla_\theta \left( -\ln(P(w_i | w_{i-(n-1)},\dots, w_{i-1})) +  \mu ||\theta||^2_2 \right)
\end{align}
where $k$ is the mini-batch size.

\paragraph{}
A single iteration of stochastic gradient descent with mini-batches has time complexity $O(M\times F \times D\times K)$ where $K$ is the mini-batch size, $M$ is the number of training examples, $D$ is the dimensionality of the feature vector and $F$ is the average number of non-zero features per example. 

\subsubsection{Adaptive Gradient Descent}
\paragraph{}
The \emph{adaptive gradient} algorithm (AdaGrad) is another improvement on gradient descent introduced in \cite{Duchi2011}. The biggest difference between gradient descent and AdaGrad is that gradient descent uses a single global learning rate, whereas AdaGrad has one for each feature. AdaGrad is fast to compute and also adaptively shrinks the step-size over time per feature. The algorithm approximates second-order gradient descent.
\paragraph{}
A single step of AdaGrad looks like:
\begin{align}
& \theta := \theta - \gamma \mbox{diag}(G_t)^{-\frac{1}{2}} g_t
\\
&G_t = \sum_{\tau=1}^t g_{\tau} g_{\tau}^T \nonumber
\\
& g_t =\nabla_\theta \left( -\ln(P(w_i | w_{i-(n-1)},\dots, w_{i-1})) +  \mu ||\theta||^2_2 \right) \nonumber
\end{align}
where diag$(x)$ is the diagonal matrix of $x$.
\paragraph{}
In this dissertation, I will be training my models using adaptive stochastic gradient descent with mini-batches.

\subsection{Model testing}
\paragraph{}
As described in Section \ref{sec:perplexity}, I will be comparing my models using their test perplexity.
\subsubsection{Perplexity}
We can fill in the equation for perplexity given by Equation \ref{eq:perplexity} now that we know our model definition:
\begin{align}
&PP(w_i | h_i)=e^{- \frac{1}{K} \sum_{i=1}^K \ln( P(w_i | h_i) ) }
\\
&\ln(P(w_i | h_i )) = \sum_j \ln(P(d_j(w_i) | q_j, h_i)) \nonumber
\\
&\ln(P(d_j(w_i) | q_j, h_i)) =  
\begin{cases}
  -\ln (1 + e^{-\hat{r}^T q_{j} -b_{j}}) & \text{if } d_j(w_i)  = 1 \\
  -\ln (1 + e^{\hat{r}^T q_{j} +b_{j}})     & \text{if } d_j(w_i) = 0
  \end{cases} \nonumber
\\
 & h_i = ( w_{i-(n-1)},\dots, w_{i-1} ) \nonumber
\end{align}
where $K$ is the number of testing examples. 


\section{Tree creation} \label{sec:treeCreation}
\paragraph{}
The binary tree used in the HLBL model can make a significant difference to the model performance and speed. In this dissertation, I explore three different types of trees.
\subsection{Huffman Trees}
\paragraph{}
\emph{Huffman coding} is an optimal prefix code created using a bottom-up agglomerative approach. Huffman coding was introduced by David Huffman in \cite{Huffman1952}. Huffman codes minimize the average number of bits needed to convey a message. In my setting, the Huffman code minimizes the average number of bits to convey a word in the vocabulary. In Huffman coding, each word has a single unique code.
\paragraph{}
I want to explore using Huffman Trees in the HLBL model because Huffman trees are guaranteed to be the most optimal code in terms of minimum average length. This will speed up the model, because on average, the model will make as few decisions as possible. Additionally, I am assuming that as the number of decisions goes up, the overall accuracy of those decisions goes down. With a Huffman code, frequent words will have short codes, therefore it will be more likely that frequent words will be correctly predicted. This should positively affect the model performance since a large percentage of words should have high probabilities of being predicted correctly. On the other hand, the most difficult, and therefore interesting, words for a language model are actually the infrequent words.  Since Huffman trees are tied to word frequencies, the decision nodes will not have any correlation to word semantics. This makes the tree harder for humans to understand and perhaps harder for the model to learn the node representations.  Huffman trees suffer from the same problem as frequency-binned classes described in Section \ref{sec:frequencybinning}.
\paragraph{}
As Huffman codes are variable length codes, it is important that Huffman codes are also prefix codes. The prefix property states that no code appears as the prefix for another code. This makes decoding possible without knowing code lengths and also makes representing the codes as a binary tree possible. 
\paragraph{}
To create a Huffman code, a probability distribution over all words must be known. Given that we have an exact unigram distribution of the training data, Huffman coding is an appealing option. To create a Huffman code, first, each word is assigned to its own cluster.  The clusters are then arranged in order of least probable to most probable. Next, the two least probable clusters are merged together recursively until there is only one cluster left. The probability of a cluster is simply the sum its constituent word probabilities. Huffman codes produce shorter codes for frequent words, and longer codes for infrequent words. Huffman codes create a single unique code per word.
\paragraph{}
The Huffman tree is created while computing the Huffman code. Each word is a leaf in the tree and each merge can be seen as a decision node in the tree. A decision node's children are the two nodes that were merged together. The Huffman code for a word is the list of binary decisions taken from the root node to the word node.

\subsubsection{Huffman Tree Pseudo-code}

\begin{algorithm}[H]
\SetAlgoLined
 \KwData{$w_1,\dots,w_{|V|}$, $P(w_1),\dots,P(w_{|V|}$)}
 \KwResult{ a single node $n_1$}
Put each word $w_1,\dots,w_{|V|}$ into its own node: $n_1,\dots,n_{|V|}$
\\ Each node $n_i$ has weight $P(w_i)$
\\Enqueue all nodes $n_1,\dots,n_m$ in $Queue1$.
\\Sort $Queue1$ in increasing order so that the smallest weight is at the head
\\ \While{ size($Queue1$) + size($Queue2$) $>$ 1} {
	$n_1$ = dequeued lowest weight node in $Queue1 \cup Queue2$
	\\$n_2$ = dequeued second lowest weight node in $Queue1 \cup Queue2$
	\\$n_3$ = new node with n1 and n2 as children,  $n_3$'s weight is $weight(n_1)+weight(n_2)$
	\\enqueue $n_3$ in rear of $Queue2$
}
$n_1$ = dequeued node from $Queue2$
\\ \Return $n_1$
\end{algorithm}

\subsubsection{Creation Complexity}
\paragraph{}
Huffman encoding takes on the order of $O\left(|V| \log |V| \right)$ calculations. The initial sorting takes $O\left(|V| \log |V|\right)$ calculations and the queue processing takes $O(|V|)$ calculations.

\subsection{Brown Clustering Trees}
\paragraph{}
\emph{Brown Clustering} clusters words into classes and is a bottom-up agglomerative technique that starts off with each word in its own class, and then successively merges the classes based on some criteria. For my purposes, I will merge classes until there is only a single class left.  Brown Clustering is introduced by Peter Brown et al. in \cite{Brown1992}.
Brown Clusters try to maximize the average mutual information of adjacent classes.
\paragraph{}
I am interested in using Brown Clustering because, by maximizing the average mutual information of adjacent classes, Brown Clusters tend to cluster words that co-occur together. Words that co-occur are often semantically similar words. Hopefully this means decisions in the Brown Clustering tree will take into account semantic differences or at least co-occurence information. This deals with the major shortfall of Huffman trees. Mnih and Hinton do consider Brown Clustering trees in \cite{MnihHinton2009}, but dismiss it due to the fact that each word only belongs to a single class. 
\paragraph{}
The average mutual information of adjacent classes I is defined as:
\begin{align}
& I(c_1,c_2) =\sum_{c_1,c_2} P(c1,c2)  \log \left( \frac{P(c_2|c_1)}{P(c_2)} \right)
\\
&P(c) = \frac{count(c)}{T} \nonumber
\\
&P(c_2|c_1) = \frac{count(c1,c2)}{\sum_c count(c_1,c)} \nonumber
\end{align}
where $count(c)$ is the number of training words in class $c$ and $T$ is the size of the training set.
\paragraph{}
Brown describes a greedy algorithm that tries to maximize $I$ of all classes: "we assign each word to a distinct class and compute the average mutual information 
between adjacent classes. We then merge that pair of classes for which the loss in 
average mutual information is least. After $V - C$ of these merges, $C$ classes remain. 
Often, we find that for classes obtained in this way the average mutual information 
can be made larger by moving some words from one class to another. Therefore, after 
having derived a set of classes from successive merges, we cycle through the vocabulary moving each word to the class for which the resulting partition has the greatest 
average mutual information. Eventually no potential reassignment of a word leads to 
a partition with greater average mutual information. At this point, we stop" \cite[pg. 472]{Brown1992}.  
Note that Brown Clustering results in each word belonging to a single cluster. Also, Brown Clusters with a mutual information criteria tend to put together words that are semantically similar.
\paragraph{}
This algorithm can be improved upon, as Percy Liang demonstrates in \cite{Liang2005}. The new algorithm is similar except that in the beginning only the top $m$ most frequent words are put into their own clusters. The next words are then merged into one of the $m$ clusters based on a similarity metric. Once all the words are in one of the $m$ clusters, the $m$ clusters are merged based on~the similarity metric.
The similarity metric used by Liang is:
\begin{align}
Quality(C) = \sum_{c,c'} I(c,c') - H
\\
H= -\sum_w P(w) \log P(w) \nonumber
\end{align}
where H is the entropy and $\sum_{c,c'} I(c,c')$ is the average mutual information over all classes.
\paragraph{}
The binary tree for Brown Clustering is created the same way as it is for Huffman coding. Each word is represented using a leaf node. A new decision node is created upon each merge. The code for every word is then the decisions taken from the root to the word.

\subsubsection{Brown-Liang Clustering Pseudo-code}
The pseudo-code is outlined in \cite{Collins2011}.
\begin{algorithm}
\SetAlgoLined
 \KwData{$w_1,\dots,w_{|V|}$}
 \KwResult{ a single cluster $c_1$}
Take the top $m$ most frequent words and put each into its own cluster: $c_1\dots,c_m$

\For{ $i=(m+1),\dots, |V|$ } {
	$c_{m+1}$ = $i$th most frequent word
	\\
	merge two clusters $c_a$ and $c_b$ such that Quality(C) is maximized
	
}
\For{ $i=1,\dots, m-1 $ } {
merge two clusters $c_a$ and $c_b$ such that Quality(C) is maximized
}
\Return $c_1$
\end{algorithm}

\subsubsection{Creation Complexity}
\paragraph{}
Calculation of the original Brown Clusters takes $O(|V|^5)$ calculations and Brown-Liang clusters requires $O(|V| m^2+n|)$ calculations where $m$ is the number of initial clusters and $n$ is the corpus length.  

\subsection{Recursive ADAPTIVE Trees} \label{sec:recursiveADAPTIVETree}
\paragraph{}
Another tree introduced in \cite{MnihHinton2009} is the ADAPTIVE tree. The ADAPTIVE tree is also mentioned in Section \ref{sec:HLBL}. The ADAPTIVE tree is created top-down by clustering on learned prediction vectors. The clustering method is a mixture of two Gaussians. The clusters are created with respect to the most responsible Gaussian, which generally creates a non-balanced tree. The mixture of two Gaussians is applied recursively until a full tree is formed. The real power behind this method comes from clustering on learned prediction vectors which are better than the unigram or co-occurance statistics that inform Huffman and Brown cluster trees. The ADAPTIVE tree creates decision nodes that separate dissimilar prediction vectors, and cluster together similar prediction vectors. 

\paragraph{}
Mnih and Hinton use learned prediction vectors from the HLBL model with a random tree. I~think starting off with a better tree such as a Huffman tree or a Brown Cluster tree for the initial HLBL model can improve the ADAPTIVE tree performance. Additionally, I think recursively training the ADAPTIVE tree, on the results from the HLBL model with an ADAPTIVE tree could be fruitful. This would result in the Recursive ADAPTIVE tree which actually has two levels of recursion. This first level of recursion comes from running the ADAPTIVE algorithm multiple times. The second level of recursion comes from running the mixture of Gaussians multiple times within the ADAPTIVE algorithm.  On the first level of recursion, by recursively adapting the tree, the tree can better capture the learned word representations, which results in better learned word representations. 
\subsubsection{Recursive ADAPTIVE Pseudo-code}

\begin{algorithm}
\SetAlgoLined
RecursiveADAPTIVE: \\
 \KwData{ HLBL model M, tree T}
 \KwResult{ Recursive ADAPTIVE tree T}

\While{ perplexity(M) keeps going down } {
 	Get the average prediction vector $\bar{\hat{r}}_{w_i}$ for each word from M
 	\\R = ($\bar{\hat{r}}_{w_1}$,\dots,$\bar{\hat{r}}_{w_m}$)
	\\T = ADAPTIVE(R,empty tree);
	\\M = train HLBL model using T
}
\Return T
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
ADAPTIVE:
 \KwData{ set of word vectors R}
 \KwResult{ tree T}
\If {$size(R) <= 2$}{
	create leaf nodes for words in R and add to T
}
\Else{
	responsibilities = GaussianMixtureModel(R)
	\\ \If{ responsibility of $Gaussian_1$ for $\bar{\hat{r}}_{w_i}$ $>$  responsibility of $Gaussian_2$ for $\bar{\hat{r}}_{w_i}$} {
		add $\bar{\hat{r}}_{w_i}$ to $R_1$
	}
	\Else{
		add $\bar{\hat{r}}_{w_i}$ to $R_2$
	}
	create a decision node $n_1$ and add to T
	\\ $n_1$.leftChild = ADAPTIVE($R_1$)
	\\ $n_1$.rightChild = ADAPTIVE($R_2$)
}
\Return T
\end{algorithm}

\begin{algorithm} \label{code:GMM}
\SetAlgoLined
GaussianMixtureModel:
\\
 \KwData{ set of word vectors R}
 \KwResult{ responsibilities $\gamma$}
Initialize means $\mu_k$, covariances $\sigma_k$ and mixing coefficients $\pi_k$
\\Evaluate the initial log likelihood
\\
\While{parameters have not converged and log likelihood has not converged}{
	E step:
	\\ $\gamma(z_{nk}) = \frac{ \pi_k \mathcal{N}(R_n|\mu_k,\sigma_k) }{\sum_{j=1}^K \pi_j \mathcal{N}( R_n| \mu_j, \sigma_j) }$
	\\ M step:
	\\ $N_k = \sum_{n=1}^N \gamma(z_{nk})$
	\\ $ \mu_k^{new} = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) R_n$
	\\ $\sigma_k^{new} = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk}) (R_n - \mu_k^{new}) (R_n - \mu_k^{new})^T $
	\\ $ \pi_k^{new} = \frac{N_k}{N}$
	\\ Evaluate the log likelihood:
	\\ $\ln( Pr(R|\mu,\sigma,\pi)) = \sum_{n=1}^N \ln \left( \sum_{k=1}^K \pi_k \mathcal{N}(R_n| \mu_k, \sigma_k) \right)$
}
\Return $\gamma$
\\
Gaussian Mixture Model pseudo-code from \cite{Bishop2006}
\end{algorithm} 

\subsubsection{Creation Complexity}
\paragraph{}
The creation complexity of a Recursive ADAPTIVE tree is $O ( |V| \times \log|V| \times G + (R-1)\times M + (R-1)\times T)$ where $R$ is the number of iterations it takes the Recursive ADAPTIVE algorithm to converge, $G$ is the number of iterations it takes the Gaussian Mixture Model to converge, $M$ is the maximum training complexity of one of the previous recursive HLBL models, and $T$ is the maximum creation complexity of one of the trees used by the recursive HLBL models. 

