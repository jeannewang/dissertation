\chapter{Related Work}
\paragraph{}
The biggest limitation of neural based language models is their slow training and testing. Subsequently, there has been a lot of work in the field dedicated to speeding up neural-based models. Methods that have been used to speed up such models include creating class-based models, creating tree-based models, using gradient approximation, truncating the vocabulary, using diagonal context matrices, and changing the training order.
\paragraph{}
The slowest part of a neural model tends to be the soft-max normalization used in the output function, due to the normalization over a large vocabulary. Large vocabulary sizes also affect the time to calculate the expectation component of the objective function gradient. In addition to vocabulary size ($|V|$), other factors that affect training times include: number of training iterations ($I$), number of training tokens ($W$), number of hidden layers (H), context-size~($N$), and word representation dimensionality ($D$). Neural network model training has time complexity $O\left( I \times W \times \left( (N-1) \times D \times H +  |V| \times H \right) \right)$ \cite{Mikolov2012}. The vocabulary size and number of training tokens are by far the largest contributors to the training time. Given that we would like to train on as many examples as possible, $W$ should not be reduced. This leaves~$|V|$ as the main component we should focus on reducing. Note, that the training complexity is linear in the size of the vocabulary.
The methods described below try to speed up one or more of the contributors to the training complexity. Though these methods are explored for a single model, the ideas can be applied to speed up other models.
\section{Class-based Model}
\paragraph{}
\begin{figure}
\Tree [. [.noun [.animal {cat}  {rabbit} {dog} {\dots} ] [.{household object} {vacuum} {broom} {\dots}  ] [.{\dots} ] ]   {\dots}  ]
\caption{Simple 3 level class based model}
\label{fig:3class}
\end{figure}

Joshua Goodman proposed a class-based method for speeding up training and testing of MaxEnt models in \cite{Goodman2001}. In a class-based language model, there are two steps. The first step is to predict the class of the word and the second step is to predict the word. Each word is assigned to a single class. 
The probability of a word is given by:
\begin{align}
P(w_i | h_i)  =&  P(class(w) | h_i) \times P(w|  h_i, class(w))
\\ h_i =& ( w_{i-(n-1)},\dots, w_{i-1} ) \nonumber
\end{align}
The constituent probabilities $P\left(class(w) | h_i \right)$ and $P \left(w|  h_i, class(w) \right)$ are predicted using two separate MaxEnt models. The MaxEnt formulation is given by Equation \ref{eq:maxent}. The MaxEnt model uses a soft-max output function which normalizes over all words in the vocabulary.
\paragraph{}
The idea is to limit the number of possible outputs, which is the same as limiting the effective size of the vocabulary. By first selecting a class, the word probability can be normalized over the class size instead of the vocabulary size. If the words are distributed uniformly across 100 classes, then the first normalization will be over 100 (for the classes) and the second normalization will occur over $\frac{1}{100}$th the vocabulary size (for the word). This can drastically reduce training times. For example, if there are 10,000 words in the vocabulary, there will be two normalizations over 100, with time proportional to 200. The normal soft-max output will normalize over the entire vocabulary in time proportional to 10,000. In this case, there is a 98\% reduction in training time. In general, there will be an order $O(\sqrt{|V|})$ speedup.
\paragraph{}
Goodman also suggested using additional prediction levels, though he found that more than three levels did not improve the model much. In a three level system such as Figure \ref{fig:3class}, for the word \emph{dog}, a model might first predict the super-class \emph{noun}, then predict the class \emph{animal}, and finally predict the word \emph{dog}. The classes and super-classes are generally semantic or lexical classes and are generated by clustering based on some similarity metric.
\subsection{Frequency-binned Classes} \label{sec:frequencybinning}
\paragraph{}
Instead of using classes that are semantically or lexically motivated, \emph{frequency-binned} classes can also be used. Words can be assigned to classes based on their relative frequency. This would still have the speed advantage of the class-based structure but would remove the overhead of creating "smart" classes. Though frequency-binning is very easy to implement, the loss of "smart" classes does have negative effects on the model performance. With a semantic class based system, words in the same class will be semantically similar. Therefore, if the wrong word is predicted for a particular class, at least the predicted word will be similar to the correct word. With frequency-binning, an incorrectly selected word could be very semantically different. Even so, Mikolov in \cite{Mikolov2012} claims frequency-binned classes work well. 

\section{Tree-based Model}
\paragraph{}
Another method to speed up the training and testing of neural models language models is to arrange the vocabulary into a decision tree structure. This way, each word becomes a series of decisions through the tree. The number of decisions is order $O(\log|V|)$.

\subsection{Hierarchical Neural Network Language Model}
\paragraph{}
The \emph{hierarchical neural probabilistic language model} (HNPLM) is introduced by Frederic Morin and Yoshua Bengio in \cite{MorinBengio2005}. This idea is an extension of the class-based method proposed by Goodman. Compared to a class-based method,  a hierarchical method increases the number of prediction class levels, and also limits the number of elements in each class. If we take this idea almost to the limit, we end up with $\log_2(|V|)$ class levels and 2 elements in each class. As described in \cite{MorinBengio2005}, the hierarchy is represented by a balanced binary tree where each node represents a class with two constituents.  A word probability is then the product of all conditional class probabilities on the path through the tree to the word node as below:
\begin{align}
&P(w_i | w_{i-(n-1)},\dots, w_{i-1})  =  \prod_{j=1}^{k} P(b_j(w_i)|node_j ,  w_{i-(n-1)},\dots, w_{i-1})
\\
&P(b_j(w_i)=1|node_j,  w_{i-(n-1)},\dots, w_{i-1}) =  \sigma(\alpha_{node_j} + \beta' \tanh(c+ Wx+ UN_{node_j})) \label{eq:hplm_sig}
\\
&node_j =b_1(w_i),\dots,b_{j-1}(w_i) \nonumber
\\ 
&\sigma(x) = \frac{1}{1+\exp(-x)} \nonumber
\end{align}
where $b_j(w_i)$ is the binary decision for word $w_i$ at node $j$, $k$ is the length of the path through the tree for word $w_i$, and $node_j$ is the sequence of bits specifying node $j$. As we can see from Equation \ref{eq:hplm_sig}, Morin's hierarchical setup is applied to the neural probabilistic model described in Section \ref{sec:nplm}. Also, the soft-max is replaced by the sigmoid function which plays the same output function role except the normalization occurs over only two outputs.

\begin{figure}
\Tree [.$N_0$ [.$N_1$ {duck}  {rabbit} ] [.$N_2$ {dog} {squirrel} ]  ]
\caption{Simple hierarchical word tree}
\label{fig:htree}
\end{figure}

\paragraph{}
The tree used in the HNPLM is created using expert knowledge from WordNet \cite{MorinBengio2005}. The tree is a balanced tree that guarantees each branch is of length $\log_2(|V|)$. This means that each word can be predicted with $\log_2(|V|)$ normalizations over two outcomes. This is in comparison to the standard NPLM model which has a single normalization over $|V|$ words. For example, using a 10,000 word vocabulary, the HNPLM makes $14$ binary decisions per word. Each decision is over 2 outcomes, for a total of 28 expressions to consider in the normalization. The NPLM, on the other hand, has to consider 10,000 expressions in the normalization. The training and testing times are proportional to the number of expressions normalized over. In this example, we see a 99.72\% reduction in time. The hierarchical set up in general produces an order $O(\frac{|V|}{\log(|V|)})$ speedup.

\paragraph{}
The word itself can also be represented by a binary code corresponding to decisions through the tree. Going left in the tree corresponds to $0$ and going right in the tree corresponds to $1$. For example, in Figure \ref{fig:htree}, the word \emph{dog} corresponds to the code $10$.

\paragraph{}
Morin and Bengio found that the HNPLM produces a less accurate model than the original NPLM but with a $250\times$ speedup. On the Brown corpus, with $|V|$=10,000 and a 900,000 token training set, the NPLM has a test perplexity of 195.3 whereas the HNPLM has a test perplexity of 220.7.

\subsection{Hierarchical Log-Bilinear Model} \label{sec:HLBL}
\paragraph{}
Andriy Mnih and Geoffry Hinton, the originators of the log-bilinear model described in Section \ref{sec:lbl},  also explored a faster model: the \emph{hierarchical log-bilinear model} (HLBL). This dissertation is based off of the HLBL work done by Mnih and Hinton. 

\paragraph{}
The HLBL model is described in \cite{MnihHinton2009}. This model uses a tree-based structure to reduce the number of normalizations needed in the LBL model. The speedup idea is the same as in the HNPLM and also results in a $O(\frac{|V|}{\log|V|})$ speedup. I am choosing to use the HLBL because it directly affects the effective vocabulary component of the training and test times. Since the vocabulary is the largest contributor to the training and test times, reducing the effective vocabulary size should be very good at speeding up the entire model.

\paragraph{}
The formulation is similar to the LBL model. The key difference is that the word probability is now a product of binary probabilities instead of a single soft-max output. Also, there is no longer a separate representation for target words, instead we have representations for nodes in the binary tree. The HLBL formulation is given by:
\begin{align}
&P(w_i | w_{i-(n-1)},\dots, w_{i-1})  \approx \prod_j P(b_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1}) \label{eq:HLBL}
\\ 
&P(b_j(w_i) = 1 | q_j, w_{i-(n-1)},\dots, w_{i-1}) =  \sigma( \hat{r}^T q_{j} +b_{j})
\\ 
&\hat{r} = \sum_{i=1}^{n-1} C_i r_{w_i} \nonumber
\end{align}
where $b_j(w_i)$ is the binary decision for word $w_i$ at node $j$, $q_j$ is the vector representation for node $j$, and $b_{j}$ is a bias term for node $j$. The $r_{w_i}$ term is the vector representation for context word $w_i$, and $C_i$ is the matrix that corresponds to the interaction between the $i$th context word and the target node.  The $\hat{r}$ term can still be thought of as a prediction vector for $w_i$. 

\paragraph{}
Much of the hierarchical formulation, intuition, and speedup is the same as the HNPLM for the HLBL. The major difference between the two is that the HNPLM has non-linearities whereas the HLBL is linear. This makes the HLBL faster as it does not need to compute non-linearities. Additionally, the prediction vector $\hat{r}$ in the HLBL can be predicted once per word, whereas the full non-linearity must be computed per decision for the HNPLM. Another difference is that the HNPLM conditions on all previous binary decisions, whereas the HLBL uses a simplifying assumption and treats each binary decision as independent. 

\paragraph{}
The biggest contribution from \cite{MnihHinton2009} is  Mnih and Hinton's exploration of using various types of trees. Mnih and Hinton showed that different types of binary trees influence both model performance and speed. The trees explored included random trees, \emph{BALANCED} trees, \emph{ADAPTIVE} trees, \emph{ADAPTIVE($\epsilon$)}, and overcomplete trees. The creation of such trees is described in~\cite{MnihHinton2009}.
The trees were all created similarly by first training a HLBL model using a random tree, and then clustering on the trained average prediction vectors $\bar{\hat{r}}$, which were treated as word representations. The clustering algorithm used was a \emph{mixture of two gaussians}. Both gaussians assigned responsibilities to each word representation. The BALANCED algorithm made splits to maintain a balanced tree, whereas the ADAPTIVE algorithm made splits based on which cluster had a higher responsibility for the word. The ADAPTIVE($\epsilon$) algorithm added the word representation to both clusters, if both clusters had responsibilities within $\epsilon$ of 0.5. Clustering was run recursively until a full binary tree was built. 

\paragraph{}
The last trees explored were $2\times$ and $4\times$ overcomplete ADAPTIVE($\epsilon=0.4$) trees. A~$2\times$~overcomplete tree has two ADAPTIVE($\epsilon$) subtrees and a $4\times$ overcomplete tree has four such subtrees. 

\paragraph{}
For the ADAPTIVE($\epsilon$) and overcomplete trees, the same word may have multiple codes. The probability of such words must be the sum over all code instances. The new word probability is given by:
\begin{align}
P(w_i | w_{i-(n-1)},\dots, w_{i-1}) = \sum_{b \in B(w_i)} \prod_j P(b_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1})
\end{align}
where $B(w_i)$ is the set of all codes for word $w_i$.

\paragraph{}
The HLBL models with various trees in decreasing order of performance are: overcomplete, ADAPTIVE($\epsilon$), ADAPTIVE, BALANCED and random. The standard LBL model performance falls somewhere between ADAPTIVE($\epsilon$) trees and overcomplete trees. All of the HLBL models (using various trees) were at least $200\times$ faster than the LBL model. 
The key takeaway is that the type of tree used makes a difference. Trees with multiple branches for difficult words can perform better than the standard LBL model and can also be faster.

\section{Gradient Approximation}
\paragraph{}
Most neural models are trained using gradient descent (or backpropogation which uses gradient descent), so speeding up the gradient calculation can also speed up training of the overall model. Regrettably, gradient approximation techniques only speed up training and not testing. 

\subsection{Importance Sampling}
\paragraph{}
\emph{Importance sampling} for training neural based models was introduced by Yoshua Bengio and Jean-S\'{e}bastion Sen\'{e}cal in \cite{BengioSenecal2003}.
The idea behind importance sampling is to approximate the negative contribution of the gradient by sampling from an approximate distribution $Q^h_i(w_i)$ for word $w_i$. The $Q^h_i(w_i)$ distribution is given by n-gram models. 

The normal gradient is computed:
\begin{align}
\frac{\partial}{\partial \theta} \log P_{\theta}(w_i| h_i) =& \frac{\partial}{\partial \theta} \hat{r} - \sum_i P_{\theta}(w_i| h_i) \frac{\partial}{\partial \theta} \hat{r_i} 
\\ \frac{\partial}{\partial \theta} \log P_{\theta}(w_i| h_i) =& \frac{\partial}{\partial \theta} \hat{r} - E_{P_{\theta}(w_i| h_i)} \left[ \frac{\partial}{\partial \theta} \hat{r} \right] \nonumber
\end{align}

Importance sampling tries to avoid computing the expectation $E_{P_{\theta}(w_i| h_i)} \left[ \frac{\partial}{\partial \theta} \hat{r} \right]$ as this requires computing $\hat{r}$ for every word in the vocabulary. Instead the gradient is estimated using:

\begin{align}
&\frac{\partial}{\partial \theta} \log P_{\theta}(w_i| h_i) \approx  \frac{\partial}{\partial \theta} \hat{r} - \frac{1}{M} \sum_{j=1}^k m(x_j) \frac{\partial}{\partial \theta} \hat{r}
\\ &m(x) =  \frac{\exp(\hat{r})}{Q^{h_i}(w_i=x)} \nonumber
\\ &M= \sum_{j=1}^k m(x_j) \nonumber
\end{align}
where $k$ is the number of samples taken from $Q^h_i(w_i)$, and $x_j$ is a sample from  $Q^h_i(w_i)$. If $k \ll |V|$ then importance sampling can be much faster than calculating the gradient.

\paragraph{}
Unfortunately, the number of samples $k$ must grow as training progresses to account for the growing variance between the $Q^h_i(w_i)$ distribution and the actual distribution. 
Another way to deal with the growing variance between the $Q^h_i(w_i)$ distribution and the actual distribution is to adapt the $Q^h_i(w_i)$ distribution throughout training as proposed in \cite{BengioSenecal2008}.
\paragraph{}
In \cite{BengioSenecal2003}, it was found that on the Brown corpus, importance sampling produces a $19\times$ speedup over the normal gradient computation.

\subsection{Noise-Contrastive Estimation}
\paragraph{}
Andriy Mnih and Yee Whye Teh use \emph{Noise-Contrastive Estimation} (NCE) for training a LBL model in \cite{MnihTeh2012}. NCE is a more stable sampling method than importance sampling. Additionally, NCE can perform well with fewer samples than importance sampling would need to perform at the same level.
The basic idea in NCE is to discriminate between samples from the actual word distribution and a known noise distribution. In \cite{MnihTeh2012}, a unigram distribution is used for the noise distribution.
The objective function $J$ in NCE is:
\begin{align}
J^h(\theta) = E_{P_d^h} \left[ \log \frac { P_{\theta}^h(w) } {P_{\theta}^h(w) + k P_n(w)} \right] + kE_{P_n} \left[ \log \frac { k P_n(w) } {P_{\theta}^h(w) + k P_n(w)} \right]
\end{align}
where $P_{\theta}^h(w)$ is the actual word distribution and $P_n(w)$ is the known noise distribution. The noise samples are $k$ times more frequent than the word samples.

The gradient can then be calculated:
\begin{align}
\frac{\partial}{\partial \theta} J^{h,w}(\theta) = \frac {  k P_n(w)  } {P_{\theta}^h(w) + k P_n(w)}  \frac{\partial}{\partial \theta}  P_{\theta}^h(w)
- \sum_{i=1}^{k} \left[ \log \frac { P_{\theta}^h(x_i) } {P_{\theta}^h(x_i) + k P_n(x_i)} \frac{\partial}{\partial \theta} \log P_{\theta}^h(x_i) \right]
\end{align}
The NCE gradient formulation's negative contribution is only a sum over $k$ samples, instead of a sum over all words $|V|$ in the vocabulary. If $k \ll |V|$ then there is a significant speedup over the normal gradient calculation.
\paragraph{}
The NCE speedup should be $O \left(\frac{nd + |V|} {nd+ k} \right)$ over the normal gradient computation. Where $n$ is the context-size and $d$ is the word-feature dimensionality.
\paragraph{}
On the Penn Treebank, Mnih and Teh found NCE to be $14\times$ faster than standard \emph{maximum likelihood} (MLE)  training. In \cite{MnihTeh2012}, it is reported that with enough samples, NCE can actually perform slightly better than MLE training on the Penn Treebank. Unfortunately, the better performance is probably due to noise or implicit regularization in NCE training. NCE training should not generally perform better than MLE training.


\section{Vocabulary Truncation}
\paragraph{}
One easy way to reduce the training and testing time, is to simply reduce the number of words. By reducing the vocabulary size, the normalization and expectation over the vocabulary becomes faster. Unfortunately the words that are generally truncated tend to be the least frequent words, which also tend to be the most difficult and interesting words for a language model to learn. 
\section{Diagonal Context Matrices}
\paragraph{}
Mnih and Hinton propose diagonal context matrices in \cite{MnihHinton2009}. Neural language models tend to have context matrices $C_i$ that capture the behavior of the context. These context matrices are of size $D \times D$, where $D$ is the dimensionality of the word embeddings. Context matrices are used in computing everything from the objective function to the gradient, so having a smaller context matrix can speedup training and testing of the whole model. One way of having a smaller context matrix is to diagonalize the matrix, so there are effectively only $D$ values in $C_i$. This makes matrix multiplication with $C_i$ faster. Note that this does reduce the representational power of large contexts. Also, the size of $C_i$ is generally much smaller than the size of $|V|$, so reducing $C_i$ will not have a major effect on training times.

\section{Training Order}
\paragraph{}
Curriculum learning proposed in \cite{Bengio2009}, can reduce the number of iterations needed in training. This reduces the overall training time. Training instances are ordered by difficulty, with simple training instances trained before difficult training instances. Intuitively, the model learns the basic structure of the language from simple training cases before tackling more difficult cases. In normal stochastic training, the training instances are fed in random order. This means that it can take longer to learn structure since easy and hard cases are interspersed with each other. Analogously, humans learn faster if we can build up a topic lesson by lesson. If we are forced to learn a hodgepodge of concepts from a topic, it takes us a much longer time to build up a full picture of the topic. Alternatively, in \cite{Mikolov2012}, Mikolov orders his training instances by "usefulness." He starts off with out-of-domain data and ends with the most important in-domain data.

