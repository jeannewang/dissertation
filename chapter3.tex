\documentclass[12pt]{ociamthesis} 

\usepackage{amssymb}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{listings}% http://ctan.org/pkg/listings
\usepackage{graphicx}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{soul}
\MakeOuterQuote{"}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\chapter{Related Work}
\paragraph{}
The main limitation of neural based language models is their long training and testing times. There has been a lot of work in the field dedicated to speeding up neural based-models. Methods that have been used to speed up such models include creating class-based models, creating tree-based models, using gradient approximation, truncating the vocabulary, and changing the training order.
\paragraph{}
The slowest part of neural models tends to be the soft-max normalization used in the output function. Additional slow areas include: \hl{stuff}.
The methods described below try to speed up one or more of these problem areas. Though these methods are explored for a single model, many of the ideas can be applied to speedup other types of models.
\section{Class based model}
\paragraph{}
Joshua Goodman proposed a class-based method for speeding up training and testing of MaxEnt models in \cite{Goodman2001}. In a class-based language model, there are two steps. The first step is to predict the class of the word and the second step is to predict the word. Each word is assigned to only one class. 
The probability of a word is given by:
\begin{align}
P(w_i | w_{i-(n-1)},\dots, w_{i-1})  =  P(class(w) | w_{i-(n-1)},\dots, w_{i-1}) \times P(w|  w_{i-(n-1)},\dots, w_{i-1}, class(w))
\end{align}
The constituent probabilities $P(class(w) | w_{i-(n-1)},\dots, w_{i-1})$ and $P(w|  w_{i-(n-1)},\dots, w_{i-1}, class(w))$ are predicted using two separate MaxEnt models. The MaxEnt formulation is given by equation \ref{eq:maxent}. The MaxEnt model uses a soft-max output function which normalizes over all words in the vocabulary.
\paragraph{}
The idea is to limit the number of possible outputs. By first selecting a class, the word probability can be normalized over the class size instead of the vocabulary size. If the words are distributed uniformly across 100 classes, then the first normalization will be over 100 (for the classes) and the second normalization will occur over $\frac{1}{100}$th the vocabulary size (for the word). This can drastically reduce training times. For example if there are 10,000 words in the vocabulary, there will be two normalizations over 100, with time proportional to 200. The normal soft-max output would normalize over the entire vocabulary for a time proportional to 10,000. In this case, there is a 98\% reduction in training time. In general there will be an order$\sqrt{|V|}$ speedup.
\paragraph{}
Goodman also suggests more prediction levels can be used, though he found that more than three levels did not lead to large improvements. In a three level system, for the word dog, a model might first predict the super-class noun, then predict the class animal, and finally predict the word dog. The classes and super-classes are generally semantic or lexical classes and are generated by clustering based on some similarity metric.
\subsection{Random classes}
\paragraph{}
Instead of using classes that are semantically or lexically motivated, random classes can also be used. Words can be assigned to one of a pre-specified number of classes randomly. This would still have the speed advantage of the class-based structure but would remove the overhead of creating "smart" classes. Mikolav in \cite{?} claims this works very well. 

\section{Tree based model}
\paragraph{}
One method to speed up training and testing neural models language models is to arrange the vocabulary into a decision tree structure. This way, each word becomes a series of decisions through the tree. The number of decisions is order $log(|V|)$.
\subsection{Hierarchical Neural Network Language Model}
The \emph{hierarchical neural probabilistic language model} (HNPLM) is introduced in \cite{MorinBengio2005}. This idea extends the class-based method proposed by Goodman. A hierarchical method extends the number of prediction class levels, and also limits the number of elements in each class. As described in \cite{MorinBengio2005}, the hierarchy is represented by a balanced binary tree where each node represents a class with two constituents.  A word probability is then the product of all class probabilities on the path through the tree to the word node. The word itself can be represented by a binary code corresponding to decisions through the tree. Going left in the tree corresponds to 0 and going right in the tree corresponds to 1.

The tree used in \cite{MorinBengio2005} is based off of expert knowledge from WordNet. The hierarchical set up produces an order $\frac{|V|}{log(|V|)}$ speedup.

\subsection{Hierarchical Log Bilinear Model}
\paragraph{}
Andriy Mnih and Geoffry Hinton the originators of the log bilinear model,which is mentioned in section \ref{sec:lbl},  also explored a faster model based off of their previous work: the \emph{hierarchical log bilinear model} (HLBL). This dissertation is largely based off of the HLBL work done by Mnih and Hinton. 

\paragraph{}
The HLBL model is described in \cite{MnihHinton2009}. This model uses a tree-based structure to reduce the number of normalizations needed in the LBL model. The speed-up idea is the same as in the HNPLM.

The formulation is very similar to the LBL model. The key difference is that the word probability is now a product of binary probabilities instead of a single soft-max output.
\begin{align}
P(w_i | w_{i-(n-1)},\dots, w_{i-1})  = 
\end{align}


\section{Gradient Approximation}
\paragraph{}
Most neural models are trained using gradient descent (or backpropogation which uses gradient descent), so speeding up the gradient calculation can also speed up training of the overall model.
\subsection{Importance Sampling}
\paragraph{}

\subsection{Noise-Contrastive Estimation}
\paragraph{}


\section{Vocabulary Truncation}
\paragraph{}
One easy way to reduce the training and testing time, is to simply reduce the number of words. By reducing the vocabulary size, the normalization over the vocabulary size becomes faster. Unfortunately the words that are generally truncated tend to be the least frequent words, which also tend to be the most interesting words for a language model to learn.

\section{Training Order}
\paragraph{}
Mikolav states in \cite{Mikolav2012} that training using training instances in a specific order can reduce the number of iterations needed in training. This reduces the overall training time. Mikolav orders his training instances by difficulty, with simple training instances trained before difficult training instances. Intuitively, the model learns the basic structure of the language from simple training cases before tackling more difficult cases. In normal training, the training instances are fed in random order. This means that it can take longer to learn structure since easy and hard cases are interspersed with each other. Analogously, humans learn faster if we can build up a topic lesson by lesson. If we are forced to learn a hodgepodge of concepts from a topic, it takes us a much longer time to build up a full picture of the topic.

\bibliography{refs} 
\bibliographystyle{plain}  %use the plain bibliography style

\end{document}
