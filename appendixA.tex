
\chapter{Derivation of HLBL Gradients}
\paragraph{}
We would like to find the gradient for the negative log likelihood with an $L_2$ regularizer with respect to its parameters $\theta$.

Given:
\begin{align*}
&-\ln(J(\theta;w_1,\dots, w_m)) = \sum_{i=1}^{m} \sum_j -\ln(P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1}))
\\ 
&-\ln(P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1})) =  
\begin{cases}
  \ln (1 + e^{-\hat{r}^T q_{j} -b_{j}}) & \text{if } d_j(w_i)  = 1 \\
  \ln (1 + e^{\hat{r}^T q_{j} +b_{j}})     & \text{if } d_j(w_i) = 0
  \end{cases}
\\
&P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1}) =  
\begin{cases}
  \sigma(\hat{r}^T q_{j} +b_{j}) & \text{if } d_j(w_i)  = 1 \\
  \sigma(-\hat{r}^T q_{j} -b_{j})     & \text{if } d_j(w_i) = 0
  \end{cases}
\\
&\hat{r} = \sum_{i=1}^{n-1} C_i r_{w_i} 
\\
&\sigma(x) = \frac{1}{1+e^{-x}}
\\
&\sigma(x) +\sigma(-x) = 1
\end{align*}

We will solve for the gradient:
\begin{align*}
&\frac{\partial}{\partial \theta} -\ln(J(\theta;w_1,\dots, w_m)) +  \mu ||\theta||^2_2
\\
& \sum_{i=1}^{m} \sum_j \frac{\partial}{\partial \theta} -\ln(P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1})) +\frac{\partial}{\partial \theta}  \mu ||\theta||^2_2
\\
& \sum_{i=1}^{m} \sum_j \frac{\partial}{\partial \theta} -\ln(P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1})) + 2 \mu \theta
\end{align*}

When $d_j(w_i)  = 1$:
\begin{align*}
& \frac{\partial}{\partial \theta} -\ln(P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1})) 
\end{align*}
\begin{align*}
&=  \frac{\partial}{\partial \theta} \ln (1 + e^{-\hat{r}^T q_{j} -b_{j}})
\\
&= \frac{1}{1 + e^{-\hat{r}^T q_{j} -b_{j}}} \frac{\partial}{\partial \theta}1 + e^{-\hat{r}^T q_{j} -b_{j}}
\\
&= \frac{e^{-\hat{r}^T q_{j} -b_{j}}}{1 + e^{-\hat{r}^T q_{j} -b_{j}}} \frac{\partial}{\partial \theta}-\hat{r}^T q_{j} -b_{j}
\\
&= \frac{-e^{-\hat{r}^T q_{j} -b_{j}}}{1 + e^{-\hat{r}^T q_{j} -b_{j}}} \frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}
\\
&= \left( \frac{1}{1 + e^{-\hat{r}^T q_{j} -b_{j}}}  - \frac{1 + e^{-\hat{r}^T q_{j} -b_{j}}}{1 + e^{-\hat{r}^T q_{j} -b_{j}}} \right) \frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}
\\
&= \left( P(d_j(w_i)=1 | q_j, w_{i-(n-1)},\dots, w_{i-1}) - 1 \right) \frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}
\end{align*}

When $d_j(w_i)  = 0$:
\begin{align*}
& \frac{\partial}{\partial \theta} -\ln(P(d_j(w_i) | q_j, w_{i-(n-1)},\dots, w_{i-1})) 
\end{align*}
\begin{align*}
&=  \frac{\partial}{\partial \theta} \ln (1 + e^{\hat{r}^T q_{j} +b_{j}})
\\
&= \frac{1}{1 + e^{\hat{r}^T q_{j} +b_{j}}} \frac{\partial}{\partial \theta}1 + e^{\hat{r}^T q_{j} +b_{j}}
\\
&= \frac{e^{\hat{r}^T q_{j} +b_{j}}}{1 + e^{\hat{r}^T q_{j} +b_{j}}} \frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}
\\
&= \frac{1}{1 + e^{-\hat{r}^T q_{j} -b_{j}}}  \frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}
\\
&= \left( P(d_j(w_i)=1 | q_j, w_{i-(n-1)},\dots, w_{i-1}) - 0 \right) \frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}
\end{align*}

$\frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}$ with respect to $b_j$:
\begin{align*}
\frac{\partial}{\partial r_j}\hat{r}^T q_{j} +b_{j} = 1
\end{align*}

$\frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}$ with respect to $q_j$:
\begin{align*}
\frac{\partial}{\partial r_j}\hat{r}^T q_{j} +b_{j} = \hat{r}
\end{align*}

$\frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}$ with respect to $r_{w_i}$:
\begin{align*}
\frac{\partial}{\partial r_{w_i}}(\sum_{i=1}^{n-1} C_i r_{w_i})^T q_{j} +b_{j} = C_i^T q_j
\end{align*}

$\frac{\partial}{\partial \theta}\hat{r}^T q_{j} +b_{j}$ with respect to $C_i$:
\begin{align*}
\frac{\partial}{\partial C_i}(\sum_{i=1}^{n-1} C_i r_{w_i})^T q_{j} +b_{j} = r_{w_i} q_j^T
\end{align*}

