
\chapter{Account of Work}
\paragraph{}
In this chapter I will describe the work carried out in this project. I will first describe engineering tasks undertaken in the course of the project. Secondly, I will define the experiments and state the experimental results. Lastly, I will analyze the results and their implications.

\section{Engineering Details}
\paragraph{}
My implementation of the HLBL model is an extension of the C++ LBL implementation by Phil Blunsom, et al. Additionally, I use tree.hh: an STL-like C++ tree library written by Kasper Peeters, and the Boost \cite{BoostSite} and Eigen \cite{eigenweb} libraries. The full implementation can be found online at \url{https://github.com/jeannewang/oxlm}. Much of the code can be found in Appendix C.
\paragraph{}
In this section, I will describe in more detail the engineering tasks necessary to build the HLBL model, described in section \ref{sec:proposedModel}. These tasks include initializing the model, building binary trees, checking the finite gradient and also caching key parameters.

\subsection{Model Initialization}
\paragraph{}
All of the model parameters ($Q,R,$ and $C_i$ for all contexts) except for the B vector are initialized using Gaussian distributions with mean $0$, and variance $0.1$.
\subsubsection{B Initialization}
\paragraph{}
Intelligent initialization of the bias matrix can improve perplexity measures. The $B$ vector contains a bias for each node in the binary tree.  The $B$ vector should be initialized with the unigram distribution of the training data. As a bias $b_j$ does not directly correlate to an individual word, $b_j$ should be initialized to the sum of the unigram probability of all words in the subtree under node $j$.  Compared to initialization with a Gaussian distribution, there is a $50\times$ improvement in initial perplexity measures. If the model's initial perplexity is lower, then fewer training iterations are generally needed. A poor initial perplexity often negatively affects the perplexity of the fully trained model.

\subsection{Building Trees}
\paragraph{}
I have described the algorithms to build specific trees in section \ref{sec:treeCreation}, but I will describe here some additional tips for building the trees. I use simple integer trees for my binary trees. My trees contain indices into the $Q$ matrix for the internal node values, and indices into the $R$ matrix for the leaf values. The indices into the $R$ matrix are also the word indices. To tell if I am looking at a word, or a node, I simply check if the node is a leaf node or not.  I found building binary trees for vocabularies to be easier going bottom-up than going top-down. This is because one can group together "similar" words much more simply by going from the words to the nodes, than the other way around. I also found that the easiest way for me to keep track of indices to the $Q$ matrix is by only storing the indices in the tree. Since the $Q$ matrix is initialized with a Gaussian, it makes no difference which $Q$ node is assigned to which $Q$ index so long as it is consistent. I just assign $Q$ indices sequentially breadth-first though the tree. 
\paragraph{}
When reading in trees from binary codes, it is necessary to build the tree top-down. To build the tree, I read in every code, and for each digit in the code, add in a node if necessary. If a new code has a branch that does not exist in the tree, I simply create it, and continue parsing the code.

\subsubsection{Mixture of Gaussians for ADAPTIVE trees}
\paragraph{}
A \emph{Gaussian Mixture Model} (GMM) is used to cluster word representations in the ADAPTIVE and Recursive ADAPTIVE trees described in section \ref{code:GMM}. To initialize the parameters for the GMM, I randomly assign the word representations into two groups, and from there calculate the group means and covariances to initialize $\mu_k$ and $\sigma_k$. The mixing coefficients $\pi_k$ are initialized to the uniform distribution. Also, when calculating the Gaussian, I found it much faster to use diagonalized versions of the covariance matrices. While this does lose some representational power of the covariance matrices, I find the speedup to be worth it. I run each GMM until convergence, or for 10 epochs, whichever is less. If any of the parameters become ill-defined, I simply back off to the previous epoch's values. 

\subsection{Finite Gradient Check}
\paragraph{}
An easy place to mess up the HLBL implementation is in the gradient computation. One easy way to check if the gradients are being calculated correctly is to use a finite gradient check. The finite gradient check uses the mathematical definition of the derivative. I am going to use the central difference definition, though the forward and backward differences can also be used. The derivative is given by:
\begin{align}
\frac{d}{d\theta} J(\theta) = \lim_{\epsilon \to 0} \frac{ J({\theta + \epsilon} )- J({\theta - \epsilon}) } {2 \epsilon}
\end{align}
We see that the derivative is given in the limit of $\epsilon \to 0$, therefore if we set $\epsilon$ to a small number ($\approx10^4$), we can approximate the derivative of the objective function. The finite gradient check can then be compared with the gradients calculated from equations \ref{eq:gradients} on the training data. The finite gradient check is computationally slow, and so should only be used as a check and not as the final gradient computation.

\subsection{Caching}
\paragraph{}
Certain variables should be cached as they are used multiples times while calculating portions of the objective function, the gradients or the perplexity. It is important to cache these values as the caching can reduce the training time by avoiding recomputing computationally-expensive values. The values that should be cached include: $\hat{r}$, and $C_i^T q_j$. $\hat{r}$ is particularly important to cache as it is used in the calculation of the objective function, all of the gradients, and the perplexity. Note that a separate $\hat{r}$ should be cached for training versus testing data.

\section{Experiments} \label{sec:experiments}
\paragraph{}
I have conducted experiments to show the performance and speed of the HLBL model with various trees. To be able to compare the models, I use the same parameter values for each model. I used a word and node representation size of $100$ dimensions, a $5$-word context (including the target word), a mini batch size of $10,000$ and a learning step-size of $0.01$. I also use a small regularization constant while training all of the models. Most of the models can be learned faster if the parameters are tuned to the individual model. The models are all trained on the same machine using a single thread. The machine runs at 3.47GHz and has 60GB of RAM.

\subsection{Datasets}
\paragraph{}
For most of my experiments, I use the Penn Treebank dataset. Specifically, I use the Wall Street Journal (WSJ) sections 2-21 for my training set and section 22 for my testing set. There are 950028 training tokens and 40117 testing tokens. The training set has been pre-processed to replace any word that occurs less than 6 times with "\_UNK\_". This reduces the vocabulary to 10531 words. The testing set has also been preprocessed to replace any word not seen in the training set with "\_UNK\_".
\subsubsection{Large Vocabulary}
\paragraph{}
Real languages have much larger vocabularies than the WSJ dataset. The Oxford English Dictionary currently has 171,476 full word entries for English \cite{OED}. For a morphologically rich language, the vocabulary size can easily exceed that of English.
\paragraph{}
The HLBL model really shines in terms of speed with large vocabulary sizes. The speedup using the HLBL model is order $O(\frac{|V|}{log|V|})$. To highlight this, I also run some experiments where I use a subset of the British National Corpus (BNC). I use the a random 120,000 lines of the BNC, where 100,000 lines are randomly assigned to the training set and the other 20,000 lines are assigned to the test set. Due to an error in generating the test set, 80,000 of the training set sentences are also in the test set. This simply means the testing perplexities will be better than they really should be for all models. Since the BNC dataset is only being used to highlight the speed difference of models, I did not rerun the experiments. There are 2141639 training tokens and 2138171 testing tokens. The training set has a vocabulary size of 84439 (including "\_UNK\_").The test set is preprocessed to replace any word not seen in the training set with "\_UNK\_". We can see on the 85K word vocabulary where non-HLBL models slow down immensely.


%WSJ sections 2-21, with words that occur only once preprocessed out, as a training set. This set has a vocabulary size of 23768. 

%\subsection{Varying Trees}
%I use random Trees, Huffman trees, Brown Cluster trees, ADAPTIVE trees, and Recursive Adaptive Trees with the HLBL model.

\section{Experimental Results}
\paragraph{}
In this section I will briefly describe the experiments and their aims, then display the results and finally analyze and compare the results.

\subsection{Binary Tree Performance by Length and Time}
\paragraph{}
The creation of the trees used in the HLBL model can be done separately from the model. We can look at the trees themselves and see if any of their traits contribute to their performance within the HLBL model.  In table \ref{tab:trees}, we can see the tree creation statistics. I will not describe tree $T_6$ in this section, as the tree is described in section \ref{sec:ADAPTIVEQ}.
 
\paragraph{}
The average code length gives us the average number of decisions that must be made per word. The training time is linear in the average code length. As we can see, the Huffman tree has the shortest average length, which is expected. But, the Recursive ADAPTIVE(2,Huffman) tree has a much larger average length. Hopefully this implies that the ADAPTIVE(2,Huffman) tree is giving up short codes for the addition of more useful decision nodes. If we delve further into the code length, we can see how code length affects the test word perplexities. In table \ref{tab:perplexityCodeLength}, we can see the breakdown of average word perplexities, of words with code length $n$, for each tree. The breakdown follows the intuition that longer codes will have worse perplexities. This is due to the increased number of decisions that must be made correctly as the code gets longer. The random tree does not stay true to this intuition, though this is perhaps because of the random tree's tendency to have subtrees that look left-deep or right-deep , whereas the other trees tend to be bushy. Figure \ref{fig:deepvsbushytrees} gives an example of left-deep, right-deep and bushy trees. The left-deep or right-deep trees have a much higher probability of going one direction than the other, so long codes may still have high probabilities. 

\begin{figure}\centering
\begin{tabular}{ccc}
\Tree [.x     [.x     [.x    {x} {\dots}  ] {\dots}  ] {\dots}  ] &
\Tree [.x   [.x [.x {x} {x} ] [.x {x} {x} ]  ]    [.x [.x {x} {x} ]  [.x {x} {x} ] ] ] &
\Tree [.x    {\dots}  [.x    {\dots}  [.x    {\dots} {x} ]  ]  ] 
\end{tabular} 
\caption{Left-deep, Bushy, and Right-deep trees}
\label{fig:deepvsbushytrees}
\end{figure}

\paragraph{}
All of the trees have as many or almost as many internal nodes as words, meaning we have full trees. A full tree is a tree in which every node, excluding leaf nodes, has two children. The fullness of the trees means that there are not wasted internal nodes that are not on the path to a word. Note that fullness is not the same as bushiness. The varying shapes of the trees (bushiness) accounts for the discrepancies in the average code length. The training time is also linear in the number of non-leaf nodes, as they correlate directly to the $R$ matrix.
\paragraph{}
We see the greatest differences in the trees in the time it takes to build them. The random tree only depends on the vocabulary so its build time has order $O(|V|)$. Oddly, in table \ref{tab:trees}, the random tree takes longer to build than the Huffman tree. The Huffman tree has creation order $O(|V|\times log|V)|$ and should be slower to build than the random tree.  The discrepancy is due to poor handling of random nodes in my code causing random trees to be built slowly. The Brown Clustering tree is by far the most time intensive tree to build. The Brown Clustering tree takes order $O(|V|^3+n|)$, where $n$ is the corpus length. It is slower to build than the Recursive ADAPTIVE($r,t$) trees which requires $r-1$ trees and $r-1$ models to be built and trained. The Recursive ADAPTIVE($r,t$) tree has creation order $O ( |V| \times log|V| \times g + (r-1)\times m + (r-1)\times t)$ where $g$ is the number of iterations it takes the Gaussian Mixture Model to converge, and $m$ is the maximum training complexity of one of the previous recursive HLBL models.

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}C{1.5cm} c C{1.5cm} C{2cm} C{2cm}@{}}\toprule 
Label & Algorithm & Mean Code Length & Non-Leaf Node Count  & Time to Build Tree\\ 
\midrule
$T_1$ & Random & 29.147 & 10530 & 1.2s\\
$T_2$ & Huffman & 26.257 & 10530 & 0.1s\\
$T_3$ & Brown Cluster & 28.388 & 10528 &271600.39s \\
$T_4$ & Recursive ADAPTIVE (3,Random)& 28.218 & 10531 &16735.63s \\
$T_5$ & Recursive ADAPTIVE (2,Huffman)& 32.254 & 10531 &10514.82s\\
$T_6$ & ADAPTIVE(word2vec) & 25.82& 10530& 2067.53s\\
\bottomrule
\end{tabular}
\caption{Trees for HLBL model. Recursive ADAPTIVE $(n,tree)$ means it was an ADAPTIVE tree recursively run $n$ times and with an initial HLBL model using $tree$. The ADAPTIVE($x$) tree is an ADAPTIVE tree that was built by clustering together $x$ representations.}
\label{tab:trees}
\end{table*}

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}cccccc@{}}\toprule
Code Length & $T_1$ & $T_2$ & $T_3$ & $T_4$ & $T_5$ \\ \midrule
 3 	&           	  &         	   &  7.26        		 &                 		&                	\\
 4 	&  1.0      	  & 9.32    	   &  20.94       		 &                 		&                	\\
 5 	&  1120.88  	  & 7.19    	   &  32.29       		 &                 		&                	\\
 6 	&  453.48   	  & 24.38   	   &  63.8864     		 &   244.59        		&   1.0          	\\
 7 	&  40.52    	  & 29.15   	   &  239.025     		 &   106.42        		&   145.95       	\\
 8 	&  13873.40 	  & 59.73   	   &  363.81      		 &   253.24        		&   303.32       	\\
 9 	&  4009.30  	  & 113.15  	   &  648.24      		 &   884.43        		&   426.96       	\\
 10	&  11439.68 	  & 256.25  	   &  1401.81     		 &   974.86        		&   1042.68      	\\
 11	&  9299.78  	  & 606.39  	   &  3044.13     		 &   1886.32       		&   2061.74      	\\
 12	&  10495.94 	  & 1377.36 	   &  6088.63     		 &   3904.36       		&   3434.53      	\\
 13	&  251634.94	  & 3645.85 	   &  12131.10    		 &   7362.55       		&   2360.03      	\\
 14	&  122953.48	  & 26992.3 	   &  25847.13    		 &   13369.65      		&   5931.19      	\\
 15	&  81351.98 	  & 22990.0 	   &  40362.92    		 &   27488.21      		&   11402.56     	\\
 16	&  18468.03 	  & 19128.9 	   &  62722.05    		 &   53891.50      		&   38567.71     	\\
 17	&  23142.77 	  & 31569.1 	   &  106630.79   		 &   116346.75     		&   58304.71     	\\
 18	&  17788.92 	  & 1.0     	   &              		 &   225282.24     		&   103076.19    	\\
 19	&  26641.27 	  &         	   &              		 &   461424.90     		&   153507.21    	\\
 20	&  42511.83 	  &         	   &              		 &   1075008.64    		&   1117195.33   	\\
 21	&  64427.15 	  &         	   &              		 &   2251914.54    		&   928355.43    	\\
 22	&  19255.16 	  &         	   &              		 &   5487420.11    		&   2653912.58   	\\
 23	&  35200.19 	  &         	   &              		 &   9891214.67    		&   7303232.64   	\\
 24	&  40986.71 	  &         	   &              		 &   25164936.17   		&   14105262.04  	\\
 25	&  11930.46 	  &         	   &              		 &   49709006.56   		&   33638785.96  	\\
 26	&  18254.02 	  &         	   &              		 &   152108176.37  		&   72852492.39  	\\
 27	&  7165.78  	  &         	   &              		 &   231516402.92  		&   246464544.21 	\\
 28	&  10265.89 	  &         	   &              		 &   168746697.15  		&   584429056.52 	\\
 29	&  5248.084 	  &         	   &              		 &   744083147.85  		&   1007277159.20	\\
 30	&  14582.42 	  &         	   &              		 &   1445162704.19 		&   2197227644.04	\\
 31	&  15846.44 	  &         	   &              		 &   2641639930.40 		&   2742412857.71	\\
 32	&  127.89   	  &         	   &              		 &   1535355556.44 		&   3348260000.0 	\\
 33	&           	  &         	   &              		 &   1.0           		&                	\\
\bottomrule
\end{tabular}
\caption{Average perplexity per code length}
\label{tab:perplexityCodeLength}
\end{table*}

\subsection{Recursive Adaptive Trees}
\paragraph{}
I ran an experiment to find the optimal level of recursion for the Recursive Adaptive Tree. The results for the Recursive Adaptive tree with an initial random tree are in table \ref{tab:recursiveAdaptive}. The best level of recursion was three with the initial random tree. It is interesting to look at the code length as the number of recursion levels go up. We see that initially, the tree has a low code length, but the average tree length jumps up sharply, on the next iteration. This correlates to a sharp drop in perplexity. I imagine the tree must be reorganizing itself with more internal nodes to help make more natural decisions. We see that after recursion level two, the average tree length starts going down again. I assume this means the opposite trade-off is now happening, in return for fewer decisions overall, the tree gets rid of potentially useful decision nodes. I also ran the same experiment with an initial Huffman tree and an initial Brown Cluster tree. The Recursive ADAPTIVE Huffman tree's best perplexity was on recursion level two. The Brown Cluster on the hand did not perform better with any level of recursion. I think this implies that the Brown Clustering already has good decision nodes (and good clusters), and that reorganizing the tree actually detracts from the original decision nodes.
\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}cccc@{}}\toprule
Recursion Level & Perplexity & Code Length\\ 
\midrule
$1$ & 118.596 & 25.70\\
$2$ & 105.981 & 33.03\\
$3$ & 104.583 & 28.22\\
$4$ & 110.969 & 25.61\\
$5$ & 109.577 & 25.39\\
\bottomrule
\end{tabular}
\caption{The effect of more recursion levels on the Recursive ADAPTIVE tree with an initial random tree}
\label{tab:recursiveAdaptive}
\end{table*}

\subsection{Varying Representation Dimensionality}
\paragraph{}
The expressivity of the model directly depends on the representation dimensionality. As the size of the vocabulary and training set grows, the expressivity of the model should also grow in order to capture the additional information given by new word combinations and patterns. Mnih and Hinton in \cite{MnihHinton2009} explore varying representation dimensionalities under 100 dimensions. They found that larger dimensions afford better model performance. Mnih and Hinton concluded that 100 dimensions are enough to capture the complexities of the APNews dataset's 17964K vocabulary. I looked at representation dimensionalities larger than 100, and found the HLBL model trained on the WSJ dataset benefited from larger word representations. The results can be found in table \ref{tab:wordRepSize}. Simply by increasing the representation size, even with a random tree, we can see an increase in performance. The increases seem to tail off around 300 dimensions.  The HLBL model may need larger representation sizes than the standard LBL model, as the node representations may need to be larger to capture the complexities of the decision tree.


\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}cccc@{}}\toprule
Word Representation Size & Perplexity using Random Tree\\ 
\midrule
$100$ & 119.339 \\
$150$ & 118.483 \\
$200$ & 113.472 \\
$250$ & 112.519 \\
$300$ & 111.102 \\
\bottomrule
\end{tabular}
\caption{The effect of word representation size on test perplexity.}
\label{tab:wordRepSize}
\end{table*}

\subsection{Comparison Against Other Models}
\paragraph{}
I also compared the HLBL model with the various trees from table \ref{tab:trees} to other language models. All of the HLBL models have the same parameters as described in section \ref{sec:experiments}. The LBL and Factored LBL models are implemented by Phil Blunsom, et al. The Factored LBL model is the frequency binned model described in section \ref{sec:frequencybinning}. The model uses 100 frequency classes. The 5-gram with modified Kneser Ney smoothing is run using the SRILM implementation \cite{Alumae2010}.
The models are allowed to run until their test perplexities stop decreasing. The results are given by table \ref{tab:languageModelComparison}. The reduction in perplexities and entropy is given by table \ref{tab:reductionPerplexity}. 

The Factored model is the best speed-up technique to compare the HLBL model to, as both techniques try to limit the effective vocabulary and also speed up both the training and testing components.

\paragraph{}
All of the HLBL models, including using a random tree, perform better than the 5-gram Kneser Ney model,  so we see that even simple neural models are more expressive than co-occurence statistics. The best proposed HLBL model uses a Brown Cluster tree. The HLBL model with a Brown Cluster tree has a 17.24\% reduction in perplexity and a 3.93\% reduction in entropy over the 5-gram model and is not that much worse in terms of performance than the LBL and the factored LBL model. However, creating the Brown Cluster tree is incredibly slow, and the total time for the model is $41\times$ greater than the time for the LBL model and $57\times$ greater than the time for the Factored model. While the Brown Cluster tree performs as well as any other tree in terms of training time per epoch and testing time, the tree creation time is extremely high. As the overall purpose of this project is to find a tree that is both fast and performs reasonably, the Brown Cluster tree, sadly, does not cut it. Probably the best tree given these criteria, is the Huffman tree. The HLBL model with the Huffman tree is $2.32\times$ faster than the LBL model but 16.19\% worse in terms of perplexity and is 3.71\% worse in terms of entropy. The Huffman tree HLBL model is also better speed-wise than the Factored model. The Huffman tree HLBL model is $1.66\times$ faster than the Factored model though it is 15.4\% worse in terms of perplexity and 3.45\% worse in terms of entropy. Still, this can be a small price to pay when it comes to large corpora with hundreds of thousands of words in their vocabulary for a $O(\frac{|V|}{log|V|})$ speed-up versus a $O(\sqrt{|V|})$ speed-up. The last two HLBL models use the Recursive ADAPTIVE algorithm. The Recursive ADAPTIVE algorithm essentially creates Brown Cluster like trees given existing representations. The resultant trees have semantically/syntactically similar words close together in the tree. At least on a vocabulary of 10K, the Recursive ADAPTIVE algorithm is faster than the Brown Clustering algorithm at creating trees. The Recursive ADAPTIVE algorithm is not fast enough however to be worth using over the simple Huffman tree. 

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}ccccC{2cm}cc@{}}\toprule
Model & Tree & Perplexity & Epochs & Training Time Per Epoch & Testing Time & Total Time\\ 
\midrule
 HLBL & $T_1$ &119.339 & 71& 40.19s & 0.57s& 2855.26s\\
 HLBL & $T_2$ &115.982 & 99& 28.93s & 0.47s & 2864.64s\\
 HLBL & $T_3$ &101.563 & 118& 28.98s & 0.49s & 275020.52s \\
 HLBL & $T_4$ &104.583 & 122& 32.48s & 0.52s & 20698.71s\\
 HLBL & $T_5$ &106.992 & 114& 37.08s & 0.58s& 14742.52\\
 LBL& - &97.21 &62 &106.98s & 21.47s& 6654.23s\\
 Factored LBL & - &98.42&101 & 47.19s & 0.56s & 4766.75s \\
 5-gram KN & - &122.752& 1 & 5.51s & 0.348s & 5.51s\\
\bottomrule
\end{tabular}
\caption{Comparison of HLBL model with various trees and other language models on WSJ dataset}
\label{tab:languageModelComparison}
\end{table*}

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}cccc@{}}\toprule
Model & Tree & Reduction in Perplexity & Reduction in Entropy\\ 
\midrule
 HLBL & $T_1$ &2.78\% & 0.58\%\\
 HLBL & $T_2$ &5.12\% & 1.17\%\\
 HLBL & $T_3$ &17.24\% & 3.93\% \\
 HLBL & $T_4$ &14.8\% & 3.32\%\\
 HLBL & $T_5$ &12.84\% & 2.85\%\\
 LBL& - & 20.81\% &4.84\% \\
 Factored LBL & - &19.82\%&4.59\%\\
\bottomrule
\end{tabular}
\caption{Reduction in perplexity and entropy compared to the 5-gram Kneser-Ney smoothed model on WSJ dataset}
\label{tab:reductionPerplexity}
\end{table*}

\subsection{Visualization} \label{sec:tsne}
\paragraph{}
To see what kinds of word representations are learned by the HLBL models, I use the t-SNE visualization technique described in \cite{Maaten2008} and implemented by Laurens van der Maaten. Using t-SNE, I visualized each context word representation in matrix $R$ on a two-dimensional map. The context word is displayed on top of the two-dimensional coordinates. This should give us an idea of the types of word similarities that are learned. Additionally, I use t-SNE to visualize words learned by the tree in the matrix $Q$. Since $Q$ vectors do not directly correspond to words, I use the $Q$ node directly above a leaf word or leaf words to represent the word, or word-pair. After t-SNE is applied to the $Q$ vectors, the word or word-pair is then displayed on top of the two-dimensional location representing the $Q$ vector. The Factored model's $R$ and $Q$ matrix are displayed as a baseline.
\paragraph{}
As the full visualization over all 10K vocabulary words is large, I have selected some areas from the Q word-cloud in figure \ref{fig:Qcloud} and some areas from the R word-cloud in figure \ref{fig:Rcloud}. A fuller version of the visualizations can be found in Appendix B. 
\paragraph{}
I found that the HLBL models, using the random tree and the Huffman tree, learn some Q word representations that make semantic or syntactic sense, whereas the models with the other three trees do not. In figure \ref{fig:Qcloud} we can see that the HLBL model with the random tree clusters together numbers in the first example, and pronouns and prepositions in the second example. The HLBL model with the Huffman tree clusters together names and places. I think this may be because the random and Huffman tree HLBL models are forced to learn word "meanings" as the tree does not automatically group together similar words. For the other three models, the trees already separate out the word meanings, and so instead of focusing on learning the word meanings, the model then focuses on learning the decision paths. The other three models learn word representations that are optimized for correctly predicting the path through the tree. Since the other three models have better perplexities than the random and Huffman tree HLBL models, I assume this is because the other three trees start off with better word clusterings than is learned by the HLBL model with random or Huffman trees. This is supported by the fact that the Q word-clouds are still uniform looking and unclustered (with some exceptions), for the random and Huffman HLBL models. Additionally, the other three models have the benefit of learning the paths better.
\paragraph{}
I found the same pattern as the Q matrices also holds for the R matrices. We see that the HLBL models with the random and Huffman trees seem to learn some semantic or syntactic patterns whereas the other three trees do not. The two examples in figure \ref{fig:Rcloud} for the random tree cluster together numbers and cluster together names. The Huffman tree seems to have learned verbs and prepositions. In the first example for the Huffman tree, it looks like the left word is a present tense verb, and the right word is a past tense verb, with the exception of 'pressing'. I think the reason behind the random and Huffman tree HLBL models learning semantic and syntactic patterns for the R matrix is the same as the reasoning behind the models learning such patterns for the Q matrix. Since the Q and R matrices are learned jointly, it makes sense that they both show the same emphasis in learning. Another observation is that the Brown Cluster tree and the Recursive Adaptive trees have round Q word-clouds. This might mean that the Q matrices for these three trees are close to multi-dimensional Gaussians, and do not help much with the models.

\begin{figure}[p]
\centering
\begin{tabular}{@{}m{2cm}ccc@{}}
$T_1$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_random_it71_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_random_it71_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_random_it71_small2.png}
\\
$T_2$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Huff_iter132_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Huff_iter132_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Huff_iter132_small2.png}
\\
$T_3$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_iter_118_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_iter_118_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_iter_118_small2.png}
\\
$T_4$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptiveR3_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptiveR3_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptiveR3_small2.png}
\\
$T_5$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive-huffR1_it114_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive-huffR1_it114_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive-huffR1_it114_small2.png}
\\
Factored Model &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_factored_it7_step0_05_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_factored_it7_step0_05_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_factored_it7_step0_05_small2.png}
\end{tabular}
\caption{Q matrices for various trees projected into two dimensions. The left-most image shows the general shape of all of the words together, and the center and right-most image show close-ups of the word-cloud.}
\label{fig:Qcloud}
\end{figure}

\begin{figure}[p]
\centering
\begin{tabular}{@{}m{2cm}ccc@{}}
$T_1$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_random_it71_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_random_it71_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_random_it71_small2.png}
\\
$T_2$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Huff_iter132_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Huff_iter132_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Huff_iter132_small2.png}
\\
$T_3$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_iter_118_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_iter_118_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_iter_118_small2.png}
\\
$T_4$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptiveR3_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptiveR3_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptiveR3_small2.png}
\\
$T_5$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive-huffR1_it114_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive-huffR1_it114_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive-huffR1_it114_small2.png}
\\
Factored Model &
\includegraphics[width=0.15\textheight]{./images/tsne/R_factored_it7_step0_05_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_factored_it7_step0_05_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_factored_it7_step0_05_small2.png}
\end{tabular}
\caption{R matrices for various trees projected into two dimensions. The left-most image shows the general shape of all of the words together, and the center and right-most image show close-ups of the word-cloud.}
\label{fig:Rcloud}
\end{figure}

\subsection{word2vec Q initialization} \label{sec:ADAPTIVEQ}
\paragraph{}
One other experiment I tried was to use better word representations to initialize $Q$. After observing that the HLBL model with the Brown Cluster tree and the Recursive ADAPTIVE trees do not seem to learn semantically or lexically motivated clusters in their Q matrices, I thought that by initializing the Q matrices better, I could push the models to learn better Q matrices. I decided to use the word2vec toolkit representations implemented by Mikolav et al to initialize my $Q$ matrix. The word2vec toolkit is based off of the ideas in \cite{Mikolov2013}. I create word2vec representations with a skip-gram model, the hierarchical soft-max, a word-window of 5, and a representation dimensionality of 100. The representation creation takes 2.309 seconds running with 12 threads for the WSJ dataset. The results for the HLBL model with the Brown Cluster tree and word2vec $Q$ initialization are given by table \ref{tab:brownWord2vec}. We see that with better $Q$ initialization, the HLBL model with a Brown Cluster tree can perform almost as well as the LBL model. The perplexity of the Brown Cluster tree is only $-0.43$ away from the LBL's perplexity on the WSJ dataset. Unfortunately, this still suffers from the long creation time of the Brown Cluster tree.
\paragraph{}
To avoid the long creation time of the Brown Cluster tree, I also tried using an ADAPTIVE tree. The ADAPTIVE tree tries to cluster together similar word representations together in the tree. If the word representations capture semantic and syntactical information, then the ADAPTIVE tree acts much like the Brown Cluster tree. To keep the tree creation time low, I did not run the ADAPTIVE algorithm recursively. Using the same word2vec representations described above, I create an ADAPTIVE tree, and also initialize the Q matrix to the word2vec representations. The results can be seen in table \ref{tab:brownWord2vec}. The ADAPTIVE tree does not perform as well as the Brown Cluster tree, but is faster to create. The ADAPTIVE tree is also faster overall than the LBL model but not the Factored LBL on the WSJ dataset.

\paragraph{}
I also tried initializing the Q matrix with the word2vec representations for the Huffman tree HLBL model with no performance gain. The results can be seen in table \ref{tab:brownWord2vec}. The Huffman tree model does a good job learning the context words even without sensible initialization. 

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{cccccc}\toprule
Tree & Perplexity & Epochs & Training Time Per Epoch & Testing Time & Total Time\\ 
\midrule
$T_2$ & 115.786 & 81 & 26.31s &0.46s & 2133.98s \\
$T_3$ & 97.64 & 115& 30.92s & 0.49s& 275022.82s\\
$T_6$& 104.477 & 83& 35.46s & 0.56s& 5013.57s\\
\bottomrule
\end{tabular}
\caption{HLBL model with Q initialized to word2vec representations on WSJ dataset.}
\label{tab:brownWord2vec}
\end{table*}

\subsection{Visualization}
\paragraph{}
I ran the t-SNE visualizations described in section \ref{sec:tsne} over the HLBL models with Q initialized to the word2vec representations. Note that I did not include the Huffman tree with word2vec Q initialization since the model does not seem to be better than its non word2vec initialized counterpart. The results can be seen in figures \ref{fig:QcloudWord2Vec} and \ref{fig:RcloudWord2Vec}. We see that the Q word-clouds make more semantically and syntactically similar clusters unlike the models without the word2vec Q initialization. This is exactly the result we were looking for. The R word-clouds remain largely unchanged. 

\begin{figure}[p]
\centering
\begin{tabular}{@{}C{2cm}ccc@{}}
$T_3$&
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_QinWord2Vec_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_QinWord2Vec_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_Brown_QinWord2Vec_small2.png}
\\
$T_6$ &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive_QinWord2Vec_RinWord2Vec_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive_QinWord2Vec_RinWord2Vec_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/Q_adaptive_QinWord2Vec_RinWord2Vec_small2.png}
\end{tabular}
\caption{Q matrices for HLBL models with Q initialized to word2vec representations The left-most image shows the general shape of all of the words together, and the center and right-most image show close-ups of the word-cloud.}
\label{fig:QcloudWord2Vec}
\end{figure}

\begin{figure}[p]
\centering
\begin{tabular}{@{}C{2cm}ccc@{}}
$T_2$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_QinWord2Vec_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_QinWord2Vec_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_Brown_QinWord2Vec_small2.png}
\\
$T_6$ &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive_QinWord2Vec_RinWord2Vec_thumb.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive_QinWord2Vec_RinWord2Vec_small1.png} &
\includegraphics[width=0.15\textheight]{./images/tsne/R_adaptive_QinWord2Vec_RinWord2Vec_small2.png}
\end{tabular}
\caption{R matrices for HLBL models with Q initialized to word2vec representations The left-most image shows the general shape of all of the words together, and the center and right-most image show close-ups of the word-cloud.}
\label{fig:RcloudWord2Vec}
\end{figure}


\section{Large Vocabulary}
\paragraph{}
The HLBL model really pulls ahead in terms of speed on corpora with large vocabularies. I ran some experiments with the 85K vocabulary BNC dataset.  I run the HLBL model with two different trees, the Huffman tree and also the ADAPTIVE(word2vec) tree with Q initialized to the word2vec representations. The BNC word2vec representations took 4.960s to create running with 12 threads. 
The Factored model that performs well in terms of perplexity and speed on the 10K WSJ vocabulary, performs much slower with the 85K BNC vocabulary. The Factored model's training and testing times scale linearly with the vocabulary size. The HLBL model on the other hand scales logarithmically. In table \ref{tab:largeVocabulary}, it becomes obvious why speed up techniques for the vocabulary component of training and testing times are necessary for computing language models over real language data. We see the HLBL models are faster both in training and testing than the Factored model. The HLBL model with the Huffman tree is $4.46\times$ faster to compute than the Factored model. Though there is a perplexity loss of $20.72\%$ and an entropy loss of $4.46\%$. The HLBL model with the ADAPTIVE(word2vec) tree and Q initialized to word2vec representations is $3.89\times$ faster to compute than the Factored model and has a perplexity loss of $9.34\%$ and an entropy loss of $1.93\%$ This makes the HLBL model with the ADAPTIVE tree and word2vec Q initialization the best of the HLBL models I explored.

%\begin{table*} \centering
%\ra{1.3}
%\begin{tabular}{@{}ccC{2cm}cc@{}}\toprule
%Model & Perplexity & Training Time Per Epoch & Testing Time & Total Time\\ 
%\midrule
% LBL&105.442 &111.05s & 21.47s & 6906.57s \\
% Factored LBL &108.802& 71.05s & 0.60s & 7887.15s\\
% HLBL with Huffman Tree &131.913& 33.26s &0.48s &3492.79s \\
%\bottomrule
%\end{tabular}
%\caption{The effect of large vocabularies on training and testing times}
%\label{tab:largeVocabulary}
%\end{table*}

\begin{table*} \centering
\ra{1.3}
\begin{tabular}{@{}C{2cm}ccccC{2cm}C{2cm}C{2cm}@{}}\toprule
Model & Tree & Q init & Perplexity & Epochs & Training Time Per Epoch & Testing Time & Total Time\\ 
\midrule
 Factored LBL & - & Gaussian &144.32& 112 & 479.37s & 72.14s & 53761.58s\\
 HLBL& $T_2$ & Gaussian &182.06& 117& 102.7s &30.18s &12047.61s \\
 HLBL & $T_6$ & word2vec &159.187& 106& 129.81s &38.32s &13803.14s \\
\bottomrule
\end{tabular}
\caption{The effect of a large vocabulary (85K) from the BNC dataset on training and testing times}
\label{tab:largeVocabulary}
\end{table*}
  
  
  
